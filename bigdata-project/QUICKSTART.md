# BigData Pipeline - Quick Start Guide

## ğŸ¯ MÃ´ Táº£

Pipeline xá»­ lÃ½ dá»¯ liá»‡u job postings theo batch:

- **MinIO**: LÆ°u trá»¯ file JSON theo ngÃ y
- **Spark**: Xá»­ lÃ½ aggregations (lÆ°Æ¡ng TB, sá»‘ job theo type)
- **Cassandra**: LÆ°u káº¿t quáº£ Ä‘Ã£ xá»­ lÃ½
- **Airflow**: Tá»± Ä‘á»™ng hÃ³a (schedule má»—i 5 phÃºt)

## âš¡ Quick Start - CHá»ˆ 3 BÆ¯á»šC!

### BÆ°á»›c 1: CÃ i Docker Desktop

Download vÃ  cÃ i: https://www.docker.com/products/docker-desktop/

- Restart mÃ¡y sau khi cÃ i
- Äá»£i Docker Desktop khá»Ÿi Ä‘á»™ng (icon xanh)

### BÆ°á»›c 2: Cháº¡y Script Setup

Má»Ÿ **PowerShell as Administrator** (chuá»™t pháº£i â†’ Run as Administrator):

```powershell
# Navigate Ä‘áº¿n thÆ° má»¥c setup
cd C:\BK_WORKSPACE\bigdata\bigdata_storage_and_proccess_job_data\bigdata-project\scripts\setup

# Cháº¡y script tá»± Ä‘á»™ng
.\run-all.ps1
```

LÃ m theo hÆ°á»›ng dáº«n trÃªn mÃ n hÃ¬nh. Script sáº½ tá»± Ä‘á»™ng:

1. CÃ i kubectl, Helm, Minikube, Python (náº¿u chÆ°a cÃ³)
2. Start Kubernetes cluster
3. Táº¡o 20 file JSON dá»¯ liá»‡u máº«u
4. Deploy táº¥t cáº£ services
5. Test Spark job
6. (Optional) Deploy Airflow

**Tá»•ng thá»i gian:** ~15-20 phÃºt

### BÆ°á»›c 3: Xem Káº¿t Quáº£

CÃ¡c service sáº½ tá»± Ä‘á»™ng port-forward:

- **MinIO Console**: http://localhost:9001 (`minioadmin` / `minioadmin`)
- **Spark UI**: http://localhost:8080
- **Airflow UI**: http://localhost:8081 (`admin` / `admin`)

Query dá»¯ liá»‡u trong Cassandra:

```powershell
kubectl exec -it cassandra-0 -- cqlsh

# Trong cqlsh:
USE metrics;
SELECT * FROM avg_salary_by_experience LIMIT 10;
SELECT * FROM jobs_by_work_type LIMIT 10;
```

## ğŸ“ Cáº¥u TrÃºc Project

```
bigdata-project/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ run-all.ps1                 # â­ CHáº Y FILE NÃ€Y
â”‚   â”‚   â”œâ”€â”€ 1-install-prerequisites.ps1 # CÃ i tools
â”‚   â”‚   â”œâ”€â”€ 2-start-minikube.ps1        # Start K8s
â”‚   â”‚   â”œâ”€â”€ 3-generate-data.ps1         # Táº¡o data
â”‚   â”‚   â”œâ”€â”€ 4-deploy-infrastructure.ps1 # Deploy services
â”‚   â”‚   â”œâ”€â”€ 5-setup-minio.ps1           # Setup MinIO
â”‚   â”‚   â”œâ”€â”€ 6-init-cassandra.ps1        # Init Cassandra
â”‚   â”‚   â”œâ”€â”€ 7-build-spark-image.ps1     # Build Spark image
â”‚   â”‚   â”œâ”€â”€ 8-test-spark-job.ps1        # Test job
â”‚   â”‚   â””â”€â”€ 9-deploy-airflow.ps1        # Deploy Airflow
â”‚   â””â”€â”€ generate_sample_data.py         # Script sinh data
â”œâ”€â”€ src/
â”‚   â””â”€â”€ spark_job.py                    # Spark job code
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ minio-config.yaml               # MinIO deployment
â”‚   â”œâ”€â”€ cassandra-config.yaml           # Cassandra deployment
â”‚   â”œâ”€â”€ spark-config.yaml               # Spark deployment
â”‚   â”œâ”€â”€ init-cassandra.cql              # Cassandra schema
â”‚   â””â”€â”€ spark-job-template.yaml         # Spark job template
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ job_processing_dag.py           # Airflow DAG
â”œâ”€â”€ Dockerfile.spark                    # Spark Docker image
â”œâ”€â”€ SETUP_FROM_SCRATCH.md               # HÆ°á»›ng dáº«n chi tiáº¿t
â””â”€â”€ DEPLOYMENT_GUIDE.md                 # Deployment guide

```

## ğŸ”§ Cháº¡y Tá»«ng BÆ°á»›c (Náº¿u Muá»‘n Kiá»ƒm SoÃ¡t)

Thay vÃ¬ `run-all.ps1`, cháº¡y tá»«ng script theo thá»© tá»±:

```powershell
cd bigdata-project\scripts\setup

# 1. CÃ i prerequisites
.\1-install-prerequisites.ps1

# 2. Start Minikube
.\2-start-minikube.ps1

# 3. Sinh dá»¯ liá»‡u
.\3-generate-data.ps1

# 4. Deploy infrastructure
.\4-deploy-infrastructure.ps1

# 5. Setup MinIO
.\5-setup-minio.ps1

# 6. Init Cassandra
.\6-init-cassandra.ps1

# 7. Build Spark image
.\7-build-spark-image.ps1

# 8. Test Spark job
.\8-test-spark-job.ps1

# 9. Deploy Airflow (optional)
.\9-deploy-airflow.ps1
```

## ğŸ› Troubleshooting

### Script bÃ¡o lá»—i "execution policy"

```powershell
Set-ExecutionPolicy -Scope Process -Force Bypass
```

### Docker khÃ´ng start Ä‘Æ°á»£c

- Má»Ÿ Docker Desktop
- Äá»£i icon mÃ u xanh á»Ÿ system tray
- Test: `docker ps`

### Minikube lá»—i

```powershell
minikube delete
minikube start --driver=docker --cpus=4 --memory=8192
```

### Pod bá»‹ pending/crash

```powershell
# Xem lá»—i
kubectl describe pod <pod-name>
kubectl logs <pod-name>

# Restart
kubectl delete pod <pod-name>
```

### Port-forward bá»‹ disconnect

```powershell
# Restart port-forward
kubectl port-forward svc/minio 9001:9001
kubectl port-forward svc/spark-master 8080:8080
kubectl port-forward svc/airflow-webserver 8081:8080 -n airflow
```

## ğŸ“Š Kiá»ƒm Tra Káº¿t Quáº£

### Check pods Ä‘ang cháº¡y

```powershell
kubectl get pods -A
```

### Xem Spark job logs

```powershell
kubectl get jobs
kubectl logs job/spark-job-2026-01-01
```

### Query Cassandra

```powershell
kubectl exec -it cassandra-0 -- cqlsh -e "SELECT * FROM metrics.avg_salary_by_experience LIMIT 5;"
```

### Xem MinIO files

VÃ o http://localhost:9001 â†’ Buckets â†’ sensor-data

## ğŸ“ Há»c ThÃªm

- **SETUP_FROM_SCRATCH.md**: HÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c
- **DEPLOYMENT_GUIDE.md**: Deployment guide cho production
- Chi tiáº¿t code: Xem comments trong cÃ¡c file Python/YAML

## ğŸ†˜ Cáº§n GiÃºp Äá»¡?

1. Check logs: `kubectl logs <pod-name>`
2. Describe pod: `kubectl describe pod <pod-name>`
3. Xem file SETUP_FROM_SCRATCH.md pháº§n Troubleshooting

---

**ChÃºc báº¡n setup thÃ nh cÃ´ng! ğŸš€**
