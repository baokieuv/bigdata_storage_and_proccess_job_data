# ğŸ“Š BigData Pipeline - Job Processing System

Pipeline xá»­ lÃ½ dá»¯ liá»‡u job postings theo batch tá»« MinIO â†’ Spark â†’ Cassandra vá»›i Airflow automation.

## ğŸš€ Quick Start

**Chá»‰ cáº§n 1 lá»‡nh:**

```powershell
# PowerShell as Administrator
cd bigdata-project\scripts\setup
.\run-all.ps1
```

âœ… **HoÃ n thÃ nh trong 15-20 phÃºt!**

ğŸ‘‰ **HÆ°á»›ng dáº«n chi tiáº¿t:** [QUICKSTART.md](QUICKSTART.md)

## ğŸ“‹ Kiáº¿n TrÃºc

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MinIO  â”‚â”€â”€â”€â”€â–¶â”‚ Spark â”‚â”€â”€â”€â”€â–¶â”‚ Cassandra â”‚â—€â”€â”€â”€â”€â”‚ Airflow â”‚
â”‚ (JSON)  â”‚     â”‚ (ETL) â”‚     â”‚ (Results) â”‚     â”‚ (Cron)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   20 files        Agg           3 tables        Every 5min
```

### Quy TrÃ¬nh Xá»­ LÃ½

1. **MinIO**: LÆ°u 20 file JSON (jobs_2026-01-01.json â†’ jobs_2026-01-20.json)
2. **Spark**: Äá»c file theo ngÃ y, tÃ­nh:
   - LÆ°Æ¡ng trung bÃ¬nh theo Experience Level
   - Sá»‘ lÆ°á»£ng job theo Work Type
3. **Cassandra**: LÆ°u káº¿t quáº£ + audit logs
4. **Airflow**: Tá»± Ä‘á»™ng trigger Spark job má»—i 5 phÃºt cho má»—i ngÃ y

## ğŸ› ï¸ Tech Stack

- **Kubernetes**: Minikube (local cluster)
- **Storage**: MinIO (S3-compatible)
- **Processing**: Apache Spark 3.5.1
- **Database**: Apache Cassandra 4.1
- **Orchestration**: Apache Airflow 2.7
- **Language**: Python 3.11, PySpark

## ğŸ“ File Quan Trá»ng

| File                                                     | MÃ´ Táº£                          |
| -------------------------------------------------------- | ------------------------------ |
| [QUICKSTART.md](QUICKSTART.md)                           | ğŸŒŸ Báº¯t Ä‘áº§u Ä‘Ã¢y - Setup tá»± Ä‘á»™ng |
| [SETUP_FROM_SCRATCH.md](SETUP_FROM_SCRATCH.md)           | HÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c   |
| [DEPLOYMENT_GUIDE.md](DEPLOYMENT_GUIDE.md)               | Deployment guide production    |
| [scripts/setup/run-all.ps1](scripts/setup/run-all.ps1)   | Script master - cháº¡y táº¥t cáº£    |
| [src/spark_job.py](src/spark_job.py)                     | Spark job xá»­ lÃ½ data           |
| [dags/job_processing_dag.py](dags/job_processing_dag.py) | Airflow DAG                    |

## ğŸ¯ TÃ­nh NÄƒng

- âœ… **Tá»± Ä‘á»™ng hÃ³a hoÃ n toÃ n**: Script setup 1-click
- âœ… **Batch Processing**: Xá»­ lÃ½ dá»¯ liá»‡u theo ngÃ y
- âœ… **Fault Tolerance**: Retry mechanism, audit logging
- âœ… **Scalable**: Spark distributed processing
- âœ… **Production-ready**: Kubernetes deployment
- âœ… **Monitoring**: Cassandra audit logs, Spark UI, Airflow UI

## ğŸ–¥ï¸ YÃªu Cáº§u Há»‡ Thá»‘ng

- **OS**: Windows 10/11
- **RAM**: 16GB minimum (32GB recommended)
- **Disk**: 50GB free space
- **CPU**: 4 cores+
- **Prerequisites**: Docker Desktop (script sáº½ tá»± cÃ i cÃ¡c tool khÃ¡c)

## ğŸ“Š Káº¿t Quáº£

Sau khi setup xong, báº¡n sáº½ cÃ³:

### 1. MinIO (http://localhost:9001)

- Bucket `sensor-data` vá»›i 20 file JSON
- Má»—i file: 50-100 job postings

### 2. Cassandra Tables

```sql
-- LÆ°Æ¡ng TB theo Experience Level
metrics.avg_salary_by_experience
  - experience_level, avg_salary, job_count, process_date

-- Sá»‘ job theo Work Type
metrics.jobs_by_work_type
  - work_type, job_count, process_date

-- Audit logs
metrics.job_processing_audit
  - process_date, status, records_read, records_written
```

### 3. Spark UI (http://localhost:8080)

- Monitor job execution
- View workers, executors

### 4. Airflow UI (http://localhost:8081)

- DAG `job_processing_pipeline`
- Runs every 5 minutes
- Processes one day at a time

## ğŸ” Example Query

```sql
-- Exec into Cassandra
kubectl exec -it cassandra-0 -- cqlsh

USE metrics;

-- LÆ°Æ¡ng TB theo experience
SELECT experience_level, avg_salary, job_count
FROM avg_salary_by_experience
WHERE process_date='2026-01-01'
ALLOW FILTERING;

-- Output:
-- Entry level     | 50000.0  | 15
-- Mid-Senior level| 95000.0  | 28
-- Director        | 145000.0 | 7
-- Unknown         | 65000.0  | 10
```

## ğŸ“ Learning Path

1. **Beginner**: Cháº¡y `run-all.ps1` â†’ Xem káº¿t quáº£ trong UI
2. **Intermediate**: Äá»c [spark_job.py](src/spark_job.py) â†’ Hiá»ƒu aggregation logic
3. **Advanced**: Modify code â†’ Add new metrics â†’ Redeploy

## ğŸ› Troubleshooting

### Quick Fixes

```powershell
# Restart táº¥t cáº£
minikube delete
cd scripts\setup
.\run-all.ps1

# Check pods
kubectl get pods -A

# View logs
kubectl logs <pod-name>

# Query Cassandra
kubectl exec -it cassandra-0 -- cqlsh
```

### Common Issues

| Lá»—i                     | Fix                                                |
| ----------------------- | -------------------------------------------------- |
| Docker not running      | Má»Ÿ Docker Desktop                                  |
| Pods pending            | Giáº£m resources trong config                        |
| Image pull error        | Check image name trong YAML                        |
| Port-forward disconnect | Re-run: `kubectl port-forward svc/minio 9001:9001` |

ğŸ‘‰ **Chi tiáº¿t:** [SETUP_FROM_SCRATCH.md](SETUP_FROM_SCRATCH.md) - Section Troubleshooting

## ğŸ“ˆ Next Steps

- [ ] Run complete pipeline (20 days)
- [ ] Add custom metrics to Spark job
- [ ] Deploy to cloud (AKS/EKS/GKE)
- [ ] Setup Grafana dashboards
- [ ] Add data quality checks
- [ ] Implement incremental processing

## ğŸ¤ Contributing

1. Fork repo
2. Create feature branch
3. Make changes
4. Test with `run-all.ps1`
5. Submit PR

## ğŸ“ License

MIT

---

**Made with â¤ï¸ for BigData learning**

**Questions?** Check [SETUP_FROM_SCRATCH.md](SETUP_FROM_SCRATCH.md) or [QUICKSTART.md](QUICKSTART.md)
