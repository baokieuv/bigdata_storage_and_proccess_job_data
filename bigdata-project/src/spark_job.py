import os
import sys

# Fix cho Windows - ƒê∆Ø·ªúNG D·∫™N CH√çNH X√ÅC
os.environ['HADOOP_HOME'] = r'C:\hadoop'
os.environ['PATH'] = os.environ['PATH'] + ';' + r'C:\hadoop\bin'

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, avg, count, sum, current_timestamp
import uuid

# Kh·ªüi t·∫°o Spark Session v·ªõi config cho MinIO v√† Cassandra
spark = SparkSession.builder \
    .appName("JobDataProcessingJSON") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.cassandra.connection.host", "localhost") \
    .config("spark.cassandra.connection.port", "9042") \
    .config("spark.jars.packages", 
            "com.datastax.spark:spark-cassandra-connector_2.12:3.5.0,"
            "org.apache.hadoop:hadoop-aws:3.3.4,"
            "com.amazonaws:aws-java-sdk-bundle:1.12.262") \
    .getOrCreate()

# ƒê·∫∑t log level ƒë·ªÉ d·ªÖ debug
spark.sparkContext.setLogLevel("WARN")

try:
    # ƒê·ªçc d·ªØ li·ªáu JSON t·ª´ MinIO v·ªõi nhi·ªÅu options
    print("üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ MinIO...")
    df = spark.read.option("multiLine", "true") \
                   .option("mode", "PERMISSIVE") \
                   .option("inferSchema", "true") \
                   .json("s3a://raw-json/job-postings.json")
    
    print(f"üìä T·ªïng s·ªë records: {df.count()}")
    print("Schema th·ª±c t·∫ø:")
    df.printSchema()
    
    print("\nüîç C√°c columns c√≥ trong data:")
    for i, col_name in enumerate(df.columns, 1):
        print(f"  {i}. {col_name}")
    
    print("\nüëÄ Sample data (3 d√≤ng ƒë·∫ßu):")
    df.show(3, truncate=100, vertical=True)
    
    if df.count() > 0:
        # Ki·ªÉm tra xem c√≥ column `max_salary` kh√¥ng
        if "max_salary" not in df.columns:
            print("\n‚ö†Ô∏è WARNING: Column 'max_salary' kh√¥ng t·ªìn t·∫°i!")
            print("T√¨m columns t∆∞∆°ng t·ª±:")
            salary_columns = [c for c in df.columns if 'salary' in c.lower()]
            print(f"  C√°c columns li√™n quan ƒë·∫øn salary: {salary_columns}")
            
            # T√¨m t·∫•t c·∫£ columns
            print("\nT·∫•t c·∫£ columns:")
            for col in df.columns:
                print(f"  - {col}: {df.select(col).dtypes[0][1]}")
            
            # N·∫øu ch·ªâ c√≥ _corrupt_record, file JSON b·ªã l·ªói
            if "_corrupt_record" in df.columns:
                print("\n‚ùå File JSON b·ªã l·ªói, ch·ªâ c√≥ _corrupt_record")
                print("M·ªôt v√†i d√≤ng l·ªói:")
                df.select("_corrupt_record").show(3, truncate=200)
                
                # Tho√°t v√¨ kh√¥ng th·ªÉ x·ª≠ l√Ω
                spark.stop()
                exit(1)
        
        # T√¨m columns th·ª±c t·∫ø cho c√°c field c·∫ßn thi·∫øt
        column_mapping = {}
        
        # T·ª± ƒë·ªông map columns d·ª±a tr√™n t√™n t∆∞∆°ng t·ª±
        for expected_col in ["max_salary", "min_salary", "normalized_salary", "views", "location", "job_id", "formatted_work_type"]:
            # T√¨m column t∆∞∆°ng t·ª±
            matching_cols = [c for c in df.columns if expected_col.lower() in c.lower()]
            if matching_cols:
                column_mapping[expected_col] = matching_cols[0]
                print(f"  Map '{expected_col}' -> '{matching_cols[0]}'")
            else:
                column_mapping[expected_col] = None
                print(f"  ‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y column cho '{expected_col}'")
        
        # T·∫°o dataframe v·ªõi columns ƒë√£ map
        df_mapped = df
        
        # ƒê·ªïi t√™n columns n·∫øu c·∫ßn
        for expected_col, actual_col in column_mapping.items():
            if actual_col and actual_col != expected_col:
                df_mapped = df_mapped.withColumnRenamed(actual_col, expected_col)
        
        print(f"\n‚úÖ Columns sau khi map: {df_mapped.columns}")
        
        # Ki·ªÉm tra columns c√≥ t·ªìn t·∫°i tr∆∞·ªõc khi cast
        existing_columns = [col for col in ["max_salary", "min_salary", "normalized_salary", "views"] 
                          if col in df_mapped.columns]
        
        if len(existing_columns) >= 2:  # C√≥ √≠t nh·∫•t 2 columns salary
            # L√†m s·∫°ch d·ªØ li·ªáu - ch·ªâ cast columns t·ªìn t·∫°i
            df_clean = df_mapped
            
            for col_name in ["max_salary", "min_salary", "normalized_salary", "views"]:
                if col_name in df_mapped.columns:
                    df_clean = df_clean.withColumn(col_name, col(col_name).cast("float"))
            
            # Fill NA cho c√°c columns t·ªìn t·∫°i
            salary_cols = [col for col in ["max_salary", "min_salary", "normalized_salary"] 
                          if col in df_mapped.columns]
            
            if salary_cols:
                df_clean = df_clean.na.fill(0, salary_cols)
            
            if "views" in df_mapped.columns:
                df_clean = df_clean.na.fill(0, ["views"])
            
            print(f"\n‚úÖ Data sau khi clean:")
            df_clean.show(3, truncate=True)
            
            # T√≠nh to√°n - ch·ªâ t√≠nh v·ªõi columns c√≥ s·∫µn
            print("\nüìà T√≠nh to√°n th·ªëng k√™...")
            
            # 1. L∆∞∆°ng trung b√¨nh (n·∫øu c√≥ normalized_salary)
            if "normalized_salary" in df_clean.columns:
                avg_salary = df_clean.agg(avg("normalized_salary").alias("avg_salary")).collect()[0]["avg_salary"]
                print(f"üí∞ L∆∞∆°ng trung b√¨nh to√†n b·ªô: {avg_salary}")
            
            # 2. Group by location (n·∫øu c√≥ location)
            if "location" in df_clean.columns:
                print("\nüìç Th·ªëng k√™ theo location:")
                agg_columns = []
                
                if "normalized_salary" in df_clean.columns:
                    agg_columns.append(avg("normalized_salary").alias("avg_salary"))
                
                if "job_id" in df_clean.columns:
                    agg_columns.append(count("job_id").alias("job_count"))
                else:
                    # D√πng count(*) n·∫øu kh√¥ng c√≥ job_id
                    agg_columns.append(count("*").alias("job_count"))
                
                if "views" in df_clean.columns:
                    agg_columns.append(sum("views").alias("total_views"))
                
                if agg_columns:
                    agg_location = df_clean.groupBy("location").agg(*agg_columns)
                    agg_location.show(truncate=False)
            
            # 3. Group by work type (n·∫øu c√≥ formatted_work_type)
            if "formatted_work_type" in df_clean.columns:
                print("\nüíº Th·ªëng k√™ theo work type:")
                agg_columns = []
                
                if "views" in df_clean.columns:
                    agg_columns.append(sum("views").alias("total_views"))
                
                if "normalized_salary" in df_clean.columns:
                    agg_columns.append(avg("normalized_salary").alias("avg_salary"))
                
                if "job_id" in df_clean.columns:
                    agg_columns.append(count("job_id").alias("job_count"))
                else:
                    agg_columns.append(count("*").alias("job_count"))
                
                if agg_columns:
                    agg_work_type = df_clean.groupBy("formatted_work_type").agg(*agg_columns)
                    agg_work_type.show(truncate=False)
                    
                    print("\nüóÑÔ∏è Chu·∫©n b·ªã l∆∞u v√†o Cassandra...")
                    
                    # Ch·ªâ l∆∞u n·∫øu Cassandra ƒëang ch·∫°y
                    try:
                        # L∆∞u agg_work_type
                        agg_work_type.withColumn("id", col("formatted_work_type")) \
                                     .withColumn("processed_at", current_timestamp()) \
                                     .write \
                                     .format("org.apache.spark.sql.cassandra") \
                                     .options(table="job_stats_by_work_type", keyspace="job_data") \
                                     .mode("append") \
                                     .save()
                        
                        print("‚úÖ ƒê√£ l∆∞u v√†o Cassandra table: job_stats_by_work_type")
                    except Exception as cassandra_error:
                        print(f"‚ö†Ô∏è Kh√¥ng th·ªÉ l∆∞u v√†o Cassandra: {cassandra_error}")
                        print("T·∫°o file CSV thay th·∫ø...")
                        agg_work_type.write.csv("output/job_stats_by_work_type.csv", header=True)
            
            print("\nüéâ X·ª≠ l√Ω ho√†n t·∫•t!")
        else:
            print("\n‚ùå Kh√¥ng ƒë·ªß columns salary ƒë·ªÉ x·ª≠ l√Ω")
            print("H√£y ki·ªÉm tra file JSON c√≥ ƒë√∫ng format kh√¥ng")
            
    else:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu trong file JSON.")
        
except Exception as e:
    print(f"‚ùå L·ªói: {e}")
    import traceback
    traceback.print_exc()

finally:
    spark.stop()
    print("üîö ƒê√£ d·ª´ng Spark session.")