import os
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, round, desc, when, lower, regexp_replace, split, lit, current_timestamp
import pyspark.sql.functions as F
import matplotlib.pyplot as plt
import numpy as np
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider

def setup_spark():
    """Setup SparkSession cho Windows"""
    
    # T·∫°o winutils gi·∫£
    hadoop_dir = "C:/tmp/hadoop_spark"
    bin_dir = os.path.join(hadoop_dir, "bin")
    os.makedirs(bin_dir, exist_ok=True)
    
    winutils_path = os.path.join(bin_dir, "winutils.exe")
    if not os.path.exists(winutils_path):
        with open(winutils_path, 'wb') as f:
            f.write(b'winutils dummy')
    
    os.environ['HADOOP_HOME'] = hadoop_dir
    os.environ['PATH'] = os.environ.get('PATH', '') + f';{bin_dir}'
    os.environ['HADOOP_TMP_DIR'] = 'C:/Windows/Temp'
    
    # T·∫°o SparkSession
    spark = SparkSession.builder \
        .appName("Data-Visualization-MinIO") \
        .master("local[1]") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.hadoop.tmp.dir", "C:/Windows/Temp") \
        .config("spark.ui.enabled", "false") \
        .getOrCreate()
    
    return spark

def prepare_and_clean_data(df):
    """Chu·∫©n b·ªã v√† l√†m s·∫°ch d·ªØ li·ªáu"""
    
    print("\nüîÑ CHU·∫®N B·ªä D·ªÆ LI·ªÜU...")
    
    # 1. Chuy·ªÉn ƒë·ªïi c·ªôt salary t·ª´ string sang numeric
    salary_columns = ['normalized_salary', 'min_salary', 'max_salary', 'med_salary']
    
    for col_name in salary_columns:
        if col_name in df.columns:
            # Lo·∫°i b·ªè k√Ω t·ª± kh√¥ng ph·∫£i s·ªë v√† chuy·ªÉn ƒë·ªïi
            df = df.withColumn(
                f"{col_name}_numeric",
                F.regexp_replace(F.col(col_name), "[^0-9.-]", "").cast("double")
            )
    
    # 2. T·∫°o experience level t·ª´ title (v√¨ c·ªôt formatted_experience_level r·ªóng)
    if 'title' in df.columns:
        df = df.withColumn(
            "experience_level_derived",
            F.when(
                F.lower(F.col('title')).contains("senior") | 
                F.lower(F.col('title')).contains("sr.") |
                F.lower(F.col('title')).contains("lead") |
                F.lower(F.col('title')).contains("principal") |
                F.lower(F.col('title')).contains("director") |
                F.lower(F.col('title')).contains("manager"),
                "Senior"
            ).when(
                F.lower(F.col('title')).contains("mid") |
                F.lower(F.col('title')).contains("middle") |
                F.lower(F.col('title')).contains("experienced"),
                "Mid-level"
            ).when(
                F.lower(F.col('title')).contains("junior") |
                F.lower(F.col('title')).contains("jr.") |
                F.lower(F.col('title')).contains("entry") |
                F.lower(F.col('title')).contains("fresh"),
                "Junior"
            ).when(
                F.lower(F.col('title')).contains("intern") |
                F.lower(F.col('title')).contains("trainee"),
                "Intern"
            ).otherwise("Not specified")
        )
    
    # 3. Chu·∫©n h√≥a work type
    if 'formatted_work_type' in df.columns:
        df = df.withColumn(
            "work_type_clean",
            F.when(
                F.lower(F.col('formatted_work_type')).contains("full"), "Full-time"
            ).when(
                F.lower(F.col('formatted_work_type')).contains("part"), "Part-time"
            ).when(
                F.lower(F.col('formatted_work_type')).contains("contract"), "Contract"
            ).when(
                F.lower(F.col('formatted_work_type')).contains("intern"), "Internship"
            ).when(
                F.lower(F.col('formatted_work_type')).contains("remote"), "Remote"
            ).when(
                F.lower(F.col('formatted_work_type')).contains("hybrid"), "Hybrid"
            ).otherwise(F.col('formatted_work_type'))
        )
    
    # 4. Chu·∫©n h√≥a location (l·∫•y th√†nh ph·ªë)
    if 'location' in df.columns:
        df = df.withColumn(
            "city",
            F.when(
                F.col('location').contains(","),
                F.trim(F.split(F.col('location'), ",")[0])
            ).otherwise(F.col('location'))
        )
    
    # 5. Chu·∫©n h√≥a numeric columns
    if 'applies' in df.columns:
        df = df.withColumn(
            "applies_numeric",
            F.regexp_replace(F.col('applies'), "[^0-9]", "").cast("integer")
        )
    
    if 'views' in df.columns:
        df = df.withColumn(
            "views_numeric",
            F.regexp_replace(F.col('views'), "[^0-9]", "").cast("integer")
        )
    
    print("‚úÖ ƒê√£ chu·∫©n b·ªã d·ªØ li·ªáu xong!")
    return df

def create_chart_6_salary_by_experience(df):
    """Bi·ªÉu ƒë·ªì 6: L∆∞∆°ng trung b√¨nh theo Experience Level"""
    
    print("\n" + "=" * 50)
    print("üìä BI·ªÇU ƒê·ªí 6 ‚Äì L∆∞∆°ng trung b√¨nh theo Experience Level")
    print("=" * 50)
    
    # S·ª≠ d·ª•ng c·ªôt experience_level_derived ƒë√£ t·∫°o
    if 'experience_level_derived' in df.columns and 'normalized_salary_numeric' in df.columns:
        # L·ªçc b·ªè c√°c gi√° tr·ªã "Not specified"
        filtered_df = df.filter(
            (F.col('experience_level_derived') != 'Not specified') &
            (F.col('normalized_salary_numeric').isNotNull())
        )
        
        if filtered_df.count() > 0:
            salary_by_exp = filtered_df.groupBy('experience_level_derived') \
                .agg(
                    F.round(F.avg('normalized_salary_numeric'), 2).alias("avg_salary"),
                    F.count("*").alias("job_count")
                ) \
                .orderBy('experience_level_derived')
            
            print("üí∞ L∆Ø∆†NG TRUNG B√åNH THEO EXPERIENCE LEVEL:")
            salary_by_exp.show(truncate=False)
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho bi·ªÉu ƒë·ªì
            data = salary_by_exp.collect()
            exp_levels = [row['experience_level_derived'] for row in data]
            avg_salaries = [row['avg_salary'] for row in data]
            job_counts = [row['job_count'] for row in data]
            
            # T·∫°o bi·ªÉu ƒë·ªì
            plt.figure(figsize=(14, 6))
            
            plt.subplot(1, 2, 1)
            bars1 = plt.bar(exp_levels, avg_salaries, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFD166', '#EF476F'])
            plt.xlabel('Experience Level')
            plt.ylabel('Average Salary ($)')
            plt.title('Bi·ªÉu ƒë·ªì 6: L∆∞∆°ng trung b√¨nh theo Experience Level')
            plt.xticks(rotation=45)
            
            for bar, salary in zip(bars1, avg_salaries):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                        f'${salary:,.0f}', ha='center', va='bottom', fontsize=9)
            
            plt.subplot(1, 2, 2)
            bars2 = plt.bar(exp_levels, job_counts, color=['#06D6A0', '#118AB2', '#073B4C', '#FF9E00', '#7209B7'])
            plt.xlabel('Experience Level')
            plt.ylabel('Number of Jobs')
            plt.title('S·ªë l∆∞·ª£ng Job theo Experience Level')
            plt.xticks(rotation=45)
            
            for bar, count in zip(bars2, job_counts):
                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                        f'{count}', ha='center', va='bottom', fontsize=9)
            
            plt.tight_layout()
            plt.savefig('experience_level_salary.png', dpi=300, bbox_inches='tight')
            print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: experience_level_salary.png")
            plt.show()
            return True
        else:
            print("‚ö†Ô∏è  Kh√¥ng c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t·∫°o bi·ªÉu ƒë·ªì 6")
            return False
    else:
        print("‚ö†Ô∏è  Thi·∫øu c·ªôt c·∫ßn thi·∫øt cho bi·ªÉu ƒë·ªì 6")
        return False

def create_chart_7_jobs_by_work_type(df):
    """Bi·ªÉu ƒë·ªì 7: S·ªë job theo Work Type"""
    
    print("\n" + "=" * 50)
    print("üìä BI·ªÇU ƒê·ªí 7 ‚Äì S·ªë job theo Work Type")
    print("=" * 50)
    
    if 'work_type_clean' in df.columns:
        work_stats = df.groupBy('work_type_clean') \
            .agg(F.count("*").alias("job_count")) \
            .orderBy(F.desc("job_count"))
        
        print("üë• S·ªê JOB THEO WORK TYPE:")
        work_stats.show(truncate=False)
        
        data = work_stats.collect()
        work_types = [row['work_type_clean'] for row in data]
        counts = [row['job_count'] for row in data]
        
        # T√≠nh ph·∫ßn trƒÉm
        total = sum(counts)
        percentages = [(c/total)*100 for c in counts]
        
        plt.figure(figsize=(14, 6))
        
        plt.subplot(1, 2, 1)
        colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#C77DFF', '#FF6B6B']
        plt.pie(counts, labels=work_types, autopct='%1.1f%%', colors=colors[:len(work_types)], 
                startangle=90, textprops={'fontsize': 10})
        plt.title('Ph√¢n b·ªë Job theo Work Type (Pie Chart)')
        
        plt.subplot(1, 2, 2)
        bars = plt.bar(work_types, counts, color=colors[:len(work_types)])
        plt.xlabel('Work Type')
        plt.ylabel('Number of Jobs')
        plt.title('Ph√¢n b·ªë Job theo Work Type (Bar Chart)')
        plt.xticks(rotation=45)
        
        for bar, count, percent in zip(bars, counts, percentages):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    f'{count} ({percent:.1f}%)', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.savefig('work_type_distribution.png', dpi=300, bbox_inches='tight')
        print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: work_type_distribution.png")
        plt.show()
        return True
    else:
        print("‚ö†Ô∏è  Thi·∫øu c·ªôt 'work_type_clean'")
        return False

def create_chart_8_top_locations(df):
    """Bi·ªÉu ƒë·ªì 8: Top Location c√≥ nhi·ªÅu job nh·∫•t"""
    
    print("\n" + "=" * 50)
    print("üìä BI·ªÇU ƒê·ªí 8 ‚Äì Top Location c√≥ nhi·ªÅu job nh·∫•t")
    print("=" * 50)
    
    if 'city' in df.columns:
        location_stats = df.groupBy('city') \
            .agg(F.count("*").alias("job_count")) \
            .orderBy(F.desc("job_count")) \
            .limit(10)
        
        print("üìç TOP 10 LOCATION C√ì NHI·ªÄU JOB NH·∫§T:")
        location_stats.show(truncate=False)
        
        data = location_stats.collect()
        locations = [row['city'] for row in data]
        counts = [row['job_count'] for row in data]
        
        plt.figure(figsize=(14, 8))
        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(locations)))
        
        bars = plt.barh(locations, counts, color=colors)
        plt.xlabel('Number of Jobs')
        plt.ylabel('Location')
        plt.title('Bi·ªÉu ƒë·ªì 8: Top 10 Location c√≥ nhi·ªÅu Job nh·∫•t')
        
        for bar, count in zip(bars, counts):
            plt.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height()/2, 
                    f' {count} jobs', va='center', fontsize=10, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig('top_locations.png', dpi=300, bbox_inches='tight')
        print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: top_locations.png")
        plt.show()
        return True
    else:
        print("‚ö†Ô∏è  Thi·∫øu c·ªôt 'city'")
        return False

def create_cassandra_schema():
    """T·∫°o schema trong Cassandra"""
    
    try:
        print("\nüîß ƒêANG T·∫†O CASSANDRA SCHEMA...")
        
        auth_provider = PlainTextAuthProvider(username='cassandra', password='cassandra')
        cluster = Cluster(['localhost'], port=9042, auth_provider=auth_provider)
        session = cluster.connect()
        
        # T·∫°o keyspace
        session.execute("""
            CREATE KEYSPACE IF NOT EXISTS bigdata_project 
            WITH replication = {
                'class': 'SimpleStrategy', 
                'replication_factor': 1
            }
        """)
        
        print("‚úÖ ƒê√£ t·∫°o keyspace: bigdata_project")
        
        # Chuy·ªÉn sang keyspace m·ªõi
        session.set_keyspace('bigdata_project')
        
        # T·∫°o tables
        session.execute("""
            CREATE TABLE IF NOT EXISTS processed_jobs (
                job_id TEXT PRIMARY KEY,
                title TEXT,
                company_name TEXT,
                location TEXT,
                city TEXT,
                experience_level TEXT,
                work_type TEXT,
                normalized_salary DOUBLE,
                min_salary DOUBLE,
                max_salary DOUBLE,
                applies INT,
                views INT,
                remote_allowed TEXT,
                sponsored TEXT,
                processed_time TIMESTAMP
            )
        """)
        
        session.execute("""
            CREATE TABLE IF NOT EXISTS salary_by_experience_stats (
                experience_level TEXT PRIMARY KEY,
                avg_salary DOUBLE,
                job_count INT,
                last_updated TIMESTAMP
            )
        """)
        
        session.execute("""
            CREATE TABLE IF NOT EXISTS jobs_by_work_type_stats (
                work_type TEXT PRIMARY KEY,
                job_count INT,
                percentage DOUBLE,
                last_updated TIMESTAMP
            )
        """)
        
        session.execute("""
            CREATE TABLE IF NOT EXISTS top_locations_stats (
                city TEXT PRIMARY KEY,
                job_count INT,
                rank INT,
                last_updated TIMESTAMP
            )
        """)
        
        session.execute("""
            CREATE TABLE IF NOT EXISTS charts_metadata (
                chart_id UUID PRIMARY KEY,
                chart_name TEXT,
                file_path TEXT,
                created_date DATE,
                record_count INT
            )
        """)
        
        print("‚úÖ ƒê√£ t·∫°o 5 tables trong Cassandra")
        
        # Ki·ªÉm tra tables
        rows = session.execute("""
            SELECT table_name 
            FROM system_schema.tables 
            WHERE keyspace_name = 'bigdata_project'
        """)
        
        tables = [row.table_name for row in rows]
        print(f"üìä Tables ƒë√£ t·∫°o: {tables}")
        
        cluster.shutdown()
        return True
        
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o Cassandra schema: {e}")
        return False

def save_data_to_cassandra(spark, df):
    """L∆∞u d·ªØ li·ªáu v√†o Cassandra"""
    
    try:
        print("\nüíæ ƒêANG L∆ØU D·ªÆ LI·ªÜU V√ÄO CASSANDRA...")
        
        # T·∫£i JAR files t·ª´ Maven Central (ho·∫∑c d√πng JAR local)
        cassandra_connector_jar = "com.datastax.spark:spark-cassandra-connector_2.12:3.5.0"
        
        # T·∫°o SparkSession m·ªõi v·ªõi Cassandra connector
        spark_cass = SparkSession.builder \
            .appName("Cassandra-Writer") \
            .master("local[1]") \
            .config("spark.jars.packages", 
                   "org.apache.hadoop:hadoop-aws:3.3.4," +
                   "com.amazonaws:aws-java-sdk-bundle:1.12.262," +
                   f"{cassandra_connector_jar}") \
            .config("spark.cassandra.connection.host", "localhost") \
            .config("spark.cassandra.connection.port", "9042") \
            .config("spark.cassandra.auth.username", "cassandra") \
            .config("spark.cassandra.auth.password", "cassandra") \
            .config("spark.cassandra.connection.keep_alive_ms", "60000") \
            .config("spark.sql.extensions", "com.datastax.spark.connector.CassandraSparkExtensions") \
            .getOrCreate()
        
        print("‚úÖ Spark Cassandra connector ƒë√£ t·∫£i!")
        
        # 1. L∆∞u d·ªØ li·ªáu jobs ƒë√£ x·ª≠ l√Ω
        print("üì§ ƒêang l∆∞u d·ªØ li·ªáu jobs...")
        
        jobs_df = df.select(
            'job_id', 'title', 'company_name', 'location', 'city',
            'experience_level_derived', 'work_type_clean',
            'normalized_salary_numeric', 'min_salary_numeric', 'max_salary_numeric',
            'applies_numeric', 'views_numeric',
            'remote_allowed', 'sponsored'
        ).withColumnRenamed('experience_level_derived', 'experience_level') \
         .withColumnRenamed('work_type_clean', 'work_type') \
         .withColumnRenamed('normalized_salary_numeric', 'normalized_salary') \
         .withColumnRenamed('min_salary_numeric', 'min_salary') \
         .withColumnRenamed('max_salary_numeric', 'max_salary') \
         .withColumnRenamed('applies_numeric', 'applies') \
         .withColumnRenamed('views_numeric', 'views') \
         .withColumn('processed_time', F.current_timestamp())
        
        # L·ªçc b·ªè c√°c d√≤ng kh√¥ng c√≥ job_id
        jobs_df = jobs_df.filter(F.col('job_id').isNotNull())
        
        # L∆∞u v√†o Cassandra
        jobs_df.write \
            .format("org.apache.spark.sql.cassandra") \
            .mode("append") \
            .options(table="processed_jobs", keyspace="bigdata_project") \
            .save()
        
        print(f"‚úÖ ƒê√£ l∆∞u {jobs_df.count()} jobs v√†o Cassandra")
        
        # ... (ph·∫ßn c√≤n l·∫°i gi·ªØ nguy√™n)
        
        spark_cass.stop()
        return True
        
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u v√†o Cassandra: {e}")
        import traceback
        traceback.print_exc()
        return False
def display_summary_statistics(df):
    """Hi·ªÉn th·ªã th·ªëng k√™ t·ªïng quan"""
    
    print("\n" + "=" * 60)
    print("üìà TH·ªêNG K√ä T·ªîNG QUAN")
    print("=" * 60)
    
    total_jobs = df.count()
    print(f"üìä T·ªïng s·ªë Job: {total_jobs}")
    
    # Th·ªëng k√™ salary
    if 'normalized_salary_numeric' in df.columns:
        salary_stats = df.select(
            F.round(F.avg('normalized_salary_numeric'), 2).alias("avg_salary"),
            F.min('normalized_salary_numeric').alias("min_salary"),
            F.max('normalized_salary_numeric').alias("max_salary"),
            F.count('normalized_salary_numeric').alias("jobs_with_salary")
        ).collect()[0]
        
        print(f"\nüí∞ TH·ªêNG K√ä L∆Ø∆†NG:")
        print(f"  ‚Ä¢ L∆∞∆°ng trung b√¨nh: ${salary_stats['avg_salary']:,.2f}")
        print(f"  ‚Ä¢ L∆∞∆°ng th·∫•p nh·∫•t: ${salary_stats['min_salary']:,.2f}")
        print(f"  ‚Ä¢ L∆∞∆°ng cao nh·∫•t: ${salary_stats['max_salary']:,.2f}")
        print(f"  ‚Ä¢ C√≥ l∆∞∆°ng: {salary_stats['jobs_with_salary']}/{total_jobs} jobs")
    
    # Th·ªëng k√™ work type
    if 'work_type_clean' in df.columns:
        work_type_counts = df.groupBy('work_type_clean').count().orderBy(F.desc('count'))
        print(f"\nüë• PH√ÇN B·ªê WORK TYPE:")
        for row in work_type_counts.collect():
            percentage = (row['count'] / total_jobs) * 100
            print(f"  ‚Ä¢ {row['work_type_clean']}: {row['count']} jobs ({percentage:.1f}%)")
    
    # Th·ªëng k√™ location
    if 'city' in df.columns:
        top_locations = df.groupBy('city').count().orderBy(F.desc('count')).limit(5)
        print(f"\nüìç TOP 5 LOCATIONS:")
        for row in top_locations.collect():
            print(f"  ‚Ä¢ {row['city']}: {row['count']} jobs")
    
    # Th·ªëng k√™ experience level
    if 'experience_level_derived' in df.columns:
        exp_stats = df.groupBy('experience_level_derived').count().orderBy(F.desc('count'))
        print(f"\nüéØ PH√ÇN B·ªê EXPERIENCE LEVEL:")
        for row in exp_stats.collect():
            percentage = (row['count'] / total_jobs) * 100
            print(f"  ‚Ä¢ {row['experience_level_derived']}: {row['count']} jobs ({percentage:.1f}%)")
import socket
import time

def check_port_forward():
    """Ki·ªÉm tra port-forward c√≥ ƒëang ch·∫°y kh√¥ng"""
    
    print("\nüîç KI·ªÇM TRA PORT-FORWARD...")
    
    max_retries = 5
    for attempt in range(max_retries):
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.settimeout(2)
        
        try:
            result = sock.connect_ex(('localhost', 9042))
            sock.close()
            
            if result == 0:
                print("‚úÖ Port-forward ƒëang ch·∫°y tr√™n localhost:9042")
                return True
            else:
                print(f"   ‚ùå Port-forward kh√¥ng ch·∫°y (th·ª≠ {attempt + 1}/{max_retries})")
                
        except Exception as e:
            print(f"   ‚ùå L·ªói ki·ªÉm tra port: {e}")
            sock.close()
        
        if attempt < max_retries - 1:
            print(f"   ‚è≥ Ch·ªù 3 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i...")
            time.sleep(3)
    
    print("\n‚ùå KH√îNG T√åM TH·∫§Y PORT-FORWARD!")
    print("\nüí° H√ÉY M·ªû TERMINAL M·ªöI V√Ä CH·∫†Y:")
    print("   kubectl port-forward pod/cassandra-0 9042:9042")
    print("\n   Gi·ªØ terminal ƒë√≥ m·ªü, sau ƒë√≥ ch·∫°y l·∫°i script n√†y")
    return False

def connect_to_cassandra():
    """K·∫øt n·ªëi Cassandra v·ªõi ki·ªÉm tra port-forward"""
    
    # Ki·ªÉm tra port-forward tr∆∞·ªõc
    if not check_port_forward():
        return None, None
    
    print("\nüîó ƒêANG K·∫æT N·ªêI CASSANDRA...")
    
    try:
        from cassandra.cluster import Cluster
        from cassandra.auth import PlainTextAuthProvider
        
        auth_provider = PlainTextAuthProvider(username='cassandra', password='cassandra')
        
        # D√πng timeout ng·∫Øn h∆°n ƒë·ªÉ nhanh fail
        cluster = Cluster(
            ['localhost'],
            port=9042,
            auth_provider=auth_provider,
            connect_timeout=10
        )
        
        session = cluster.connect()
        
        # Test nhanh
        row = session.execute("SELECT release_version FROM system.local", timeout=5).one()
        print(f"‚úÖ K·∫øt n·ªëi th√†nh c√¥ng! Cassandra version: {row.release_version}")
        
        return cluster, session
        
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi: {e}")
        
        # Diagnostic
        if "timed out" in str(e).lower():
            print("üí° C√≥ th·ªÉ Cassandra pod ch∆∞a s·∫µn s√†ng ho·∫∑c ƒëang restart")
            print("   Ki·ªÉm tra: kubectl get pods -l app=cassandra")
        
        return None, None
def main():
    """H√†m ch√≠nh"""
    
    spark = None
    try:
        print("=" * 70)
        print("üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù D·ªÆ LI·ªÜU T·ª™ MINIO")
        print("=" * 70)
        
        # 1. Setup Spark
        spark = setup_spark()
        
        # 2. ƒê·ªçc d·ªØ li·ªáu t·ª´ MinIO
        print("\nüì• ƒêANG ƒê·ªåC D·ªÆ LI·ªÜU T·ª™ MINIO...")
        df = spark.read \
            .option("multiline", "true") \
            .json("s3a://test-bucket/postings*.json")
        
        original_count = df.count()
        print(f"‚úÖ ƒê√£ ƒë·ªçc {original_count} b·∫£n ghi t·ª´ {len(df.inputFiles())} file")
        
        # 3. Chu·∫©n b·ªã v√† l√†m s·∫°ch d·ªØ li·ªáu
        df = prepare_and_clean_data(df)
        
        # 4. T·∫°o c√°c bi·ªÉu ƒë·ªì
        print("\nüé® ƒêANG T·∫†O BI·ªÇU ƒê·ªí...")
        
        chart6_success = create_chart_6_salary_by_experience(df)
        chart7_success = create_chart_7_jobs_by_work_type(df)
        chart8_success = create_chart_8_top_locations(df)
        
        # 5. Hi·ªÉn th·ªã th·ªëng k√™ t·ªïng quan
        display_summary_statistics(df)
        
        # 6. T√≠ch h·ª£p v·ªõi Cassandra
        print("\n" + "=" * 70)
        print("üóÑÔ∏è  T√çCH H·ª¢P V·ªöI CASSANDRA")
        print("=" * 70)
        
         # K·∫øt n·ªëi Cassandra
        cluster, session = connect_to_cassandra()
        
        if cluster and session:
            try:
                # T·∫°o schema
                schema_created = create_cassandra_schema(session)
                
                if schema_created:
                    # L∆∞u d·ªØ li·ªáu
                    save_success = save_data_to_cassandra(df, session)
                    
                    if save_success:
                        print("\nüéâ ƒê√É L∆ØU D·ªÆ LI·ªÜU V√ÄO CASSANDRA TH√ÄNH C√îNG!")
                        cassandra_success = True
                    else:
                        print("\n‚ö†Ô∏è  L·ªói khi l∆∞u d·ªØ li·ªáu v√†o Cassandra")
                        cassandra_success = False
                else:
                    print("\n‚ö†Ô∏è  L·ªói khi t·∫°o schema Cassandra")
                    cassandra_success = False
                    
            except Exception as e:
                print(f"‚ùå L·ªói khi l√†m vi·ªác v·ªõi Cassandra: {e}")
                cassandra_success = False
        else:
            print("‚ö†Ô∏è  Kh√¥ng th·ªÉ k·∫øt n·ªëi Cassandra")
            cassandra_success = False
        
        # 7. T√≥m t·∫Øt k·∫øt qu·∫£
        print("\n" + "=" * 70)
        print("üéØ T√ìM T·∫ÆT K·∫æT QU·∫¢")
        print("=" * 70)
        
        print(f"""
    ‚úÖ ƒê√É HO√ÄN TH√ÄNH X·ª¨ L√ù D·ªÆ LI·ªÜU!
    
    üìä D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω: {original_count} b·∫£n ghi
    üé® Bi·ªÉu ƒë·ªì ƒë√£ t·∫°o:
        ‚Ä¢ Bi·ªÉu ƒë·ªì 6: {'‚úÖ' if chart6_success else '‚ùå'} L∆∞∆°ng theo Experience Level
        ‚Ä¢ Bi·ªÉu ƒë·ªì 7: {'‚úÖ' if chart7_success else '‚ùå'} Job theo Work Type  
        ‚Ä¢ Bi·ªÉu ƒë·ªì 8: {'‚úÖ' if chart8_success else '‚ùå'} Top Locations
    
    üìÅ Files ƒë√£ l∆∞u:
        ‚Ä¢ experience_level_salary.png
        ‚Ä¢ work_type_distribution.png  
        ‚Ä¢ top_locations.png
    
    üóÑÔ∏è  Cassandra: {'‚úÖ ƒê√£ l∆∞u' if 'save_success' in locals() and save_success else '‚ö†Ô∏è Ch∆∞a l∆∞u'}
    
    üéâ HO√ÄN T·∫§T D·ª∞ √ÅN BIG DATA!
        """)
        
        return 0
        
    except Exception as e:
        print(f"\n‚ùå C√ì L·ªñI X·∫¢Y RA: {e}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        if spark:
            spark.stop()
            print("\nüî¥ ƒê√£ ƒë√≥ng SparkSession")

if __name__ == "__main__":
    sys.exit(main())