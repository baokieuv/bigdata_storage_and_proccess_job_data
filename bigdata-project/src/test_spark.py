import os
import sys
import tempfile
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, round, desc, when
import pyspark.sql.functions as F  # Th√™m d√≤ng n√†y
import matplotlib.pyplot as plt
import numpy as np

def setup_spark():
    """Setup SparkSession cho Windows"""
    
    # T·∫°o winutils gi·∫£
    hadoop_dir = "C:/tmp/hadoop_spark"
    bin_dir = os.path.join(hadoop_dir, "bin")
    os.makedirs(bin_dir, exist_ok=True)
    
    winutils_path = os.path.join(bin_dir, "winutils.exe")
    if not os.path.exists(winutils_path):
        with open(winutils_path, 'wb') as f:
            f.write(b'winutils dummy')
    
    os.environ['HADOOP_HOME'] = hadoop_dir
    os.environ['PATH'] = os.environ.get('PATH', '') + f';{bin_dir}'
    os.environ['HADOOP_TMP_DIR'] = 'C:/Windows/Temp'
    
    # T·∫°o SparkSession
    spark = SparkSession.builder \
        .appName("Data-Visualization-MinIO") \
        .master("local[1]") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.hadoop.tmp.dir", "C:/Windows/Temp") \
        .config("spark.ui.enabled", "false") \
        .getOrCreate()
    
    return spark

def read_data_from_minio(spark, bucket="test-bucket"):
    """ƒê·ªçc d·ªØ li·ªáu t·ª´ MinIO"""
    
    print("üì• ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ MinIO...")
    
    # ƒê·ªçc t·∫•t c·∫£ file JSON
    df = spark.read \
        .option("multiline", "true") \
        .option("inferSchema", "true") \
        .json(f"s3a://{bucket}/postings*.json")
    
    print(f"‚úÖ ƒê√£ ƒë·ªçc {df.count()} b·∫£n ghi t·ª´ {len(df.inputFiles())} file")
    print(f"üìã S·ªë c·ªôt: {len(df.columns)}")
    
    # Hi·ªÉn th·ªã schema ƒë·ªÉ ki·ªÉm tra
    print("\nüìÑ SCHEMA D·ªÆ LI·ªÜU:")
    df.printSchema()
    
    return df

def analyze_and_visualize(df):
    """Ph√¢n t√≠ch d·ªØ li·ªáu v√† t·∫°o visualizations"""
    
    print("\n" + "=" * 60)
    print("üìä B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH D·ªÆ LI·ªÜU")
    print("=" * 60)
    
    # Ki·ªÉm tra c√°c c·ªôt c√≥ s·∫µn
    print("\nüîç C√ÅC C·ªòT C√ì S·∫¥N:")
    for i, column in enumerate(df.columns, 1):
        print(f"  {i:2d}. {column}")
    
    # T·∫°o mapping gi·ªØa t√™n c·ªôt th·ª±c t·∫ø v√† t√™n c·ªôt y√™u c·∫ßu
    column_mapping = {}
    
    # T√¨m c√°c c·ªôt t∆∞∆°ng ·ª©ng
    for col_name in df.columns:
        col_lower = col_name.lower()
        
        if 'experience' in col_lower and 'level' in col_lower:
            column_mapping['formatted_experience_level'] = col_name
        elif 'salary' in col_lower:
            if 'normalized' in col_lower or 'salary' == col_lower:
                column_mapping['normalized_salary'] = col_name
        elif 'work' in col_lower and 'type' in col_lower:
            column_mapping['formatted_work_type'] = col_name
        elif 'location' in col_lower:
            column_mapping['location'] = col_name
    
    print(f"\nüîç MAPPING C·ªòT T√åM ƒê∆Ø·ª¢C:")
    for key, value in column_mapping.items():
        print(f"  ‚Ä¢ {key} -> {value}")
    
    # BI·ªÇU ƒê·ªí 6: L∆∞∆°ng trung b√¨nh theo Experience Level
    print("\n" + "-" * 50)
    print("üìä BI·ªÇU ƒê·ªí 6 ‚Äì L∆∞∆°ng trung b√¨nh theo Experience Level")
    print("-" * 50)
    
    if 'formatted_experience_level' in column_mapping and 'normalized_salary' in column_mapping:
        exp_col = column_mapping['formatted_experience_level']
        salary_col = column_mapping['normalized_salary']
        
        # T√≠nh l∆∞∆°ng trung b√¨nh theo experience level
        salary_by_exp = df.groupBy(exp_col) \
            .agg(
                round(avg(salary_col), 2).alias("avg_salary"),
                F.count("*").alias("job_count")
            ) \
            .orderBy(exp_col)
        
        print("üí∞ L∆Ø∆†NG TRUNG B√åNH THEO EXPERIENCE LEVEL:")
        salary_by_exp.show(truncate=False)
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu cho bi·ªÉu ƒë·ªì
        exp_levels = [row[exp_col] for row in salary_by_exp.collect()]
        avg_salaries = [row["avg_salary"] for row in salary_by_exp.collect()]
        job_counts = [row["job_count"] for row in salary_by_exp.collect()]
        
        # T·∫°o bi·ªÉu ƒë·ªì 1
        plt.figure(figsize=(12, 6))
        
        # Subplot 1: Bar chart l∆∞∆°ng trung b√¨nh
        plt.subplot(1, 2, 1)
        bars = plt.bar(exp_levels, avg_salaries, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])
        plt.xlabel('Experience Level')
        plt.ylabel('Average Salary')
        plt.title('Bi·ªÉu ƒë·ªì 6: L∆∞∆°ng trung b√¨nh theo Experience Level')
        plt.xticks(rotation=45)
        
        # Th√™m gi√° tr·ªã l√™n c√°c c·ªôt
        for bar, salary in zip(bars, avg_salaries):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    f'{salary:.2f}', ha='center', va='bottom', fontsize=9)
        
        # Subplot 2: Bar chart s·ªë l∆∞·ª£ng job
        plt.subplot(1, 2, 2)
        bars2 = plt.bar(exp_levels, job_counts, color=['#FFD166', '#06D6A0', '#118AB2', '#EF476F'])
        plt.xlabel('Experience Level')
        plt.ylabel('Number of Jobs')
        plt.title('S·ªë l∆∞·ª£ng Job theo Experience Level')
        plt.xticks(rotation=45)
        
        # Th√™m gi√° tr·ªã l√™n c√°c c·ªôt
        for bar, count in zip(bars2, job_counts):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    f'{count}', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.savefig('experience_level_salary.png', dpi=300, bbox_inches='tight')
        print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: experience_level_salary.png")
        
    else:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y c·ªôt 'formatted_experience_level' ho·∫∑c 'normalized_salary'")
    
    # BI·ªÇU ƒê·ªí 7: S·ªë job theo Work Type
    print("\n" + "-" * 50)
    print("üìä BI·ªÇU ƒê·ªí 7 ‚Äì S·ªë job theo Work Type")
    print("-" * 50)
    
    if 'formatted_work_type' in column_mapping:
        work_type_col = column_mapping['formatted_work_type']
        
        # ƒê·∫øm s·ªë job theo work type
        jobs_by_work_type = df.groupBy(work_type_col) \
            .agg(F.count("*").alias("job_count")) \
            .orderBy(desc("job_count"))
        
        print("üë• S·ªê JOB THEO WORK TYPE:")
        jobs_by_work_type.show(truncate=False)
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        work_types = [row[work_type_col] for row in jobs_by_work_type.collect()]
        work_type_counts = [row["job_count"] for row in jobs_by_work_type.collect()]
        
        # T√≠nh ph·∫ßn trƒÉm
        total_jobs = sum(work_type_counts)
        percentages = [(count/total_jobs)*100 for count in work_type_counts]
        
        # T·∫°o bi·ªÉu ƒë·ªì 2
        plt.figure(figsize=(12, 6))
        
        # Subplot 1: Pie chart
        plt.subplot(1, 2, 1)
        colors = ['#FF9999', '#66B2FF', '#99FF99', '#FFCC99', '#FF99CC']
        wedges, texts, autotexts = plt.pie(
            work_type_counts, 
            labels=work_types, 
            colors=colors[:len(work_types)],
            autopct='%1.1f%%',
            startangle=90
        )
        plt.title('Bi·ªÉu ƒë·ªì 7: Ph√¢n b·ªë Job theo Work Type (Pie Chart)')
        
        # Subplot 2: Bar chart
        plt.subplot(1, 2, 2)
        bars = plt.bar(work_types, work_type_counts, color=colors[:len(work_types)])
        plt.xlabel('Work Type')
        plt.ylabel('Number of Jobs')
        plt.title('Ph√¢n b·ªë Job theo Work Type (Bar Chart)')
        plt.xticks(rotation=45)
        
        # Th√™m gi√° tr·ªã v√† ph·∫ßn trƒÉm l√™n c√°c c·ªôt
        for bar, count, percent in zip(bars, work_type_counts, percentages):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, 
                    f'{count}\n({percent:.1f}%)', ha='center', va='bottom', fontsize=9)
        
        plt.tight_layout()
        plt.savefig('work_type_distribution.png', dpi=300, bbox_inches='tight')
        print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: work_type_distribution.png")
        
    else:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y c·ªôt 'formatted_work_type'")
    
    # BI·ªÇU ƒê·ªí 8: Top Location c√≥ nhi·ªÅu job nh·∫•t
    print("\n" + "-" * 50)
    print("üìä BI·ªÇU ƒê·ªí 8 ‚Äì Top Location c√≥ nhi·ªÅu job nh·∫•t")
    print("-" * 50)
    
    if 'location' in column_mapping:
        location_col = column_mapping['location']
        
        # ƒê·∫øm s·ªë job theo location v√† l·∫•y top 10
        jobs_by_location = df.groupBy(location_col) \
            .agg(F.count("*").alias("job_count")) \
            .orderBy(desc("job_count")) \
            .limit(10)
        
        print("üìç TOP LOCATION C√ì NHI·ªÄU JOB NH·∫§T:")
        jobs_by_location.show(truncate=False)
        
        # Chu·∫©n b·ªã d·ªØ li·ªáu
        locations = [row[location_col] for row in jobs_by_location.collect()]
        location_counts = [row["job_count"] for row in jobs_by_location.collect()]
        
        # T·∫°o bi·ªÉu ƒë·ªì 3
        plt.figure(figsize=(14, 8))
        
        # T·∫°o gradient m√†u
        colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(locations)))
        
        # Horizontal bar chart
        bars = plt.barh(locations, location_counts, color=colors)
        plt.xlabel('Number of Jobs')
        plt.ylabel('Location')
        plt.title('Bi·ªÉu ƒë·ªì 8: Top 10 Location c√≥ nhi·ªÅu Job nh·∫•t')
        
        # Th√™m gi√° tr·ªã l√™n c√°c c·ªôt
        for bar, count in zip(bars, location_counts):
            plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, 
                    f' {count}', va='center', fontsize=10)
        
        plt.tight_layout()
        plt.savefig('top_locations.png', dpi=300, bbox_inches='tight')
        print("‚úÖ ƒê√£ l∆∞u bi·ªÉu ƒë·ªì: top_locations.png")
        
    else:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y c·ªôt 'location'")
    
    # T·ªïng h·ª£p th·ªëng k√™
    print("\n" + "=" * 60)
    print("üìà T·ªîNG H·ª¢P TH·ªêNG K√ä")
    print("=" * 60)
    
    total_jobs = df.count()
    print(f"üìä T·ªïng s·ªë Job: {total_jobs}")
    
    # N·∫øu c√≥ salary column, th√™m th·ªëng k√™
    if 'normalized_salary' in column_mapping:
        salary_col = column_mapping['normalized_salary']
        salary_stats = df.select(
            round(avg(salary_col), 2).alias("avg_salary"),
            round(avg(when(col(salary_col).isNotNull(), col(salary_col)).otherwise(0)), 2).alias("avg_salary_non_null")
        ).collect()[0]
        
        print(f"üí∞ L∆∞∆°ng trung b√¨nh: {salary_stats['avg_salary']}")
        print(f"üí∞ L∆∞∆°ng trung b√¨nh (non-null): {salary_stats['avg_salary_non_null']}")
    
    # Hi·ªÉn th·ªã t·∫•t c·∫£ bi·ªÉu ƒë·ªì
    plt.show()
    
    return column_mapping

def save_processed_data(spark, df, column_mapping):
    """Ch·ªâ hi·ªÉn th·ªã k·∫øt qu·∫£, kh√¥ng l∆∞u file (tr√°nh winutils error)"""
    
    print("\nüíæ K·∫æT QU·∫¢ X·ª¨ L√ù (KH√îNG L∆ØU FILE ƒê·ªÇ TR√ÅNH WINUTILS ERROR):")
    
    # 1. Th·ªëng k√™ l∆∞∆°ng theo experience level
    if 'formatted_experience_level' in column_mapping and 'normalized_salary' in column_mapping:
        exp_col = column_mapping['formatted_experience_level']
        salary_col = column_mapping['normalized_salary']
        
        salary_by_exp = df.groupBy(exp_col) \
            .agg(
                round(avg(salary_col), 2).alias("avg_salary"),
                F.count("*").alias("job_count")
            ) \
            .orderBy(exp_col)
        
        print("\nüìä L∆Ø∆†NG THEO EXPERIENCE LEVEL:")
        salary_by_exp.show(truncate=False)
    
    # 2. Th·ªëng k√™ job theo work type
    if 'formatted_work_type' in column_mapping:
        work_type_col = column_mapping['formatted_work_type']
        
        work_type_stats = df.groupBy(work_type_col) \
            .agg(F.count("*").alias("job_count")) \
            .orderBy(F.desc("job_count"))
        
        print("\nüìä JOB THEO WORK TYPE:")
        work_type_stats.show(truncate=False)
    
    # 3. Th·ªëng k√™ job theo location
    if 'location' in column_mapping:
        location_col = column_mapping['location']
        
        location_stats = df.groupBy(location_col) \
            .agg(F.count("*").alias("job_count")) \
            .orderBy(F.desc("job_count")) \
            .limit(10)
        
        print("\nüìç TOP 10 LOCATION C√ì NHI·ªÄU JOB NH·∫§T:")
        location_stats.show(truncate=False)
    
    print("\n‚úÖ K·∫øt qu·∫£ ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† hi·ªÉn th·ªã th√†nh c√¥ng!")
    print("üìà Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh file PNG")
    
    return True  # Thay v√¨ l∆∞u file
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider

def test_cassandra_connection():
    try:
        # Thay ƒë·ªïi th√¥ng tin k·∫øt n·ªëi theo config c·ªßa b·∫°n
        auth_provider = PlainTextAuthProvider(
            username='cassandra', 
            password='cassandra'
        )
        
        cluster = Cluster(
            ['localhost'],  # Ho·∫∑c IP Cassandra service
            port=9042,
            auth_provider=auth_provider
        )
        
        session = cluster.connect()
        
        print("‚úÖ K·∫øt n·ªëi Cassandra th√†nh c√¥ng!")
        
        # Ki·ªÉm tra keyspace
        rows = session.execute("SELECT keyspace_name FROM system_schema.keyspaces")
        keyspaces = [row.keyspace_name for row in rows]
        
        print(f"üìÅ Keyspaces c√≥ s·∫µn: {keyspaces}")
        
        # Ki·ªÉm tra n·∫øu keyspace c·ªßa b·∫°n t·ªìn t·∫°i
        target_keyspace = "bigdata_project"
        if target_keyspace in keyspaces:
            print(f"‚úÖ Keyspace '{target_keyspace}' t·ªìn t·∫°i")
            
            # Ki·ªÉm tra tables
            session.set_keyspace(target_keyspace)
            rows = session.execute("SELECT table_name FROM system_schema.tables WHERE keyspace_name = %s", [target_keyspace])
            tables = [row.table_name for row in rows]
            print(f"üìä Tables trong keyspace: {tables}")
        else:
            print(f"‚ö†Ô∏è Keyspace '{target_keyspace}' kh√¥ng t·ªìn t·∫°i")
            print("T·∫°o keyspace v·ªõi:")
            print(f"CREATE KEYSPACE IF NOT EXISTS {target_keyspace} WITH replication = {{'class': 'SimpleStrategy', 'replication_factor': 1}};")
        
        cluster.shutdown()
        return True
    
    except Exception as e:
        print(f"‚ùå L·ªói k·∫øt n·ªëi Cassandra: {e}")
        return False
def main():
    """H√†m ch√≠nh"""
    
    spark = None
    try:
        print("=" * 60)
        print("üöÄ B·∫ÆT ƒê·∫¶U X·ª¨ L√ù D·ªÆ LI·ªÜU T·ª™ MINIO")
        print("=" * 60)
        
        # 1. Setup Spark
        spark = setup_spark()
        
        # 2. ƒê·ªçc d·ªØ li·ªáu t·ª´ MinIO
        df = read_data_from_minio(spark)
        
        # 3. Ph√¢n t√≠ch v√† t·∫°o bi·ªÉu ƒë·ªì
        column_mapping = analyze_and_visualize(df)
        
        # 4. L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        save_processed_data(spark, df, column_mapping)
        
        print("\n" + "=" * 60)
        print("üéâ HO√ÄN T·∫§T X·ª¨ L√ù V√Ä VISUALIZATION!")
        print("=" * 60)
        
        return 0
        
    except Exception as e:
        print(f"\n‚ùå C√ì L·ªñI X·∫¢Y RA: {e}")
        import traceback
        traceback.print_exc()
        return 1
        
    finally:
        # ƒê√≥ng SparkSession
        if spark:
            spark.stop()
            print("\nüî¥ ƒê√£ ƒë√≥ng SparkSession")

if __name__ == "__main__":
    sys.exit(main())