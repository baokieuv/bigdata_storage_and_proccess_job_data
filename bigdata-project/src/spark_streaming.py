"""
Spark Streaming Job: Kafka -> Spark Streaming -> Elasticsearch -> Kibana
Luá»“ng xá»­ lÃ½: 
    - Äá»c dá»¯ liá»‡u streaming tá»« Kafka topic
    - Transform vÃ  enrich dá»¯ liá»‡u
    - Ghi vÃ o Elasticsearch Ä‘á»ƒ visualize trÃªn Kibana
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from datetime import datetime
import json
import time

# ==================== Cáº¤U HÃŒNH ====================
KAFKA_BOOTSTRAP_SERVERS = "my-cluster-kafka-bootstrap.default.svc.cluster.local:9092"
KAFKA_TOPIC = "sensor-topic"
ELASTICSEARCH_NODES = "elasticsearch"
ELASTICSEARCH_PORT = "9200"
ELASTICSEARCH_INDEX = "sensor-data"
CHECKPOINT_LOCATION = "/tmp/spark-checkpoint-kafka-es"

# Schema cho dá»¯ liá»‡u tá»« Kafka (format tá»« producer.py)
schema = StructType([
    StructField("timestamp", StringType(), nullable=False),
    StructField("counter", IntegerType(), nullable=False)
])


def create_spark_session():
    """
    Táº¡o SparkSession vá»›i cÃ¡c config cáº§n thiáº¿t cho Kafka vÃ  Elasticsearch
    Dá»±a trÃªn pattern tá»« cassandra_es_sync.py vÃ  cÃ¡c file config
    """
    spark = SparkSession.builder \
        .appName("KafkaToElasticsearchStreaming") \
        .config("spark.jars.packages", 
                "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,"
                "org.elasticsearch:elasticsearch-spark-30_2.12:8.12.0") \
        .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_LOCATION) \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("es.nodes", ELASTICSEARCH_NODES) \
        .config("es.port", ELASTICSEARCH_PORT) \
        .config("es.index.auto.create", "true") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark


def process_streaming_data(df, epoch_id):
    """
    Xá»­ lÃ½ tá»«ng batch dá»¯ liá»‡u tá»« Kafka
    Transform vÃ  enrich dá»¯ liá»‡u, sau Ä‘Ã³ ghi vÃ o Elasticsearch
    Pattern tÆ°Æ¡ng tá»± transform_load.py nhÆ°ng Ä‘Æ¡n giáº£n hÃ³a cho streaming
    """
    if df.isEmpty():
        print(f"Batch {epoch_id}: No data to process")
        return
    
    record_count = df.count()
    print(f"Batch {epoch_id}: Processing {record_count} records")
    
    try:
        # Parse JSON tá»« value column (Kafka tráº£ vá» binary)
        df_parsed = df.select(
            col("key").cast("string"),
            col("value").cast("string"),
            col("timestamp").alias("kafka_timestamp"),
            col("partition"),
            col("offset")
        ).select(
            from_json(col("value"), schema).alias("data"),
            col("kafka_timestamp"),
            col("partition"),
            col("offset")
        ).select(
            "data.*",
            col("kafka_timestamp"),
            col("partition"),
            col("offset")
        )
        
        # Enrich dá»¯ liá»‡u: thÃªm cÃ¡c trÆ°á»ng metadata vÃ  tÃ­nh toÃ¡n
        df_enriched = df_parsed.withColumn(
            "processed_at", 
            current_timestamp()
        ).withColumn(
            "date",
            to_date(col("processed_at"))
        ).withColumn(
            "hour",
            hour(col("processed_at"))
        ).withColumn(
            "day_of_week",
            date_format(col("processed_at"), "EEEE")
        ).withColumn(
            "id",
            concat(
                lit(f"batch_{epoch_id}_"),
                col("partition"),
                lit("_"),
                col("offset")
            )
        ).withColumn(
            "timestamp_parsed",
            # Thá»­ parse vá»›i nhiá»u format khÃ¡c nhau (ISO format tá»« producer)
            coalesce(
                to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss.SSSSSS"),
                to_timestamp(col("timestamp"), "yyyy-MM-dd'T'HH:mm:ss"),
                to_timestamp(col("timestamp"), "yyyy-MM-dd HH:mm:ss"),
                to_timestamp(col("timestamp"))
            )
        ).drop("timestamp").withColumnRenamed("timestamp_parsed", "event_timestamp")
        
        # Hiá»ƒn thá»‹ sample data Ä‘á»ƒ debug
        print(f"\n--- Batch {epoch_id} Sample Data ---")
        df_enriched.select("id", "event_timestamp", "counter", "processed_at", "hour").show(5, truncate=False)
        
        # TÃ­nh toÃ¡n thá»‘ng kÃª
        stats = df_enriched.agg(
            count("*").alias("total_records"),
            sum("counter").alias("total_counter"),
            avg("counter").alias("avg_counter"),
            max("counter").alias("max_counter"),
            min("counter").alias("min_counter")
        ).collect()[0]
        
        print(f"Stats - Total: {stats['total_records']}, "
              f"Sum: {stats['total_counter']}, "
              f"Avg: {stats['avg_counter']:.2f}")
        
        # Ghi vÃ o Elasticsearch (pattern tá»« cassandra_es_sync.py)
        df_enriched.write \
            .format("org.elasticsearch.spark.sql") \
            .option("es.resource", f"{ELASTICSEARCH_INDEX}/_doc") \
            .option("es.nodes", ELASTICSEARCH_NODES) \
            .option("es.port", ELASTICSEARCH_PORT) \
            .option("es.index.auto.create", "true") \
            .option("es.mapping.id", "id") \
            .option("es.write.operation", "index") \
            .mode("append") \
            .save()
        
        print(f"âœ“ Batch {epoch_id}: Successfully wrote {record_count} records to Elasticsearch index '{ELASTICSEARCH_INDEX}'")
        print("-" * 60)
        
    except Exception as e:
        print(f"âœ— Batch {epoch_id}: Error processing data - {str(e)}")
        import traceback
        traceback.print_exc()
        # KhÃ´ng throw exception Ä‘á»ƒ streaming tiáº¿p tá»¥c cháº¡y


def main():
    """
    HÃ m chÃ­nh Ä‘á»ƒ cháº¡y Spark Streaming job
    Luá»“ng: Kafka -> Spark Streaming -> Elasticsearch -> Kibana
    """
    print("=" * 70)
    print("ğŸš€ Starting Kafka to Elasticsearch Streaming Job")
    print("=" * 70)
    print(f"Kafka: {KAFKA_BOOTSTRAP_SERVERS}")
    print(f"Topic: {KAFKA_TOPIC}")
    print(f"Elasticsearch: {ELASTICSEARCH_NODES}:{ELASTICSEARCH_PORT}")
    print(f"Index: {ELASTICSEARCH_INDEX}")
    print("=" * 70)
    
    spark = None
    query = None
    
    try:
        # Táº¡o SparkSession
        print("\nğŸ“¦ Creating SparkSession...")
        spark = create_spark_session()
        print("âœ“ SparkSession created successfully")
        
        # Äá»c streaming tá»« Kafka (retry logic tÆ°Æ¡ng tá»± producer.py)
        print(f"\nğŸ“¡ Connecting to Kafka...")
        max_retries = 5
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                df_kafka = spark \
                    .readStream \
                    .format("kafka") \
                    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
                    .option("subscribe", KAFKA_TOPIC) \
                    .option("startingOffsets", "latest") \
                    .option("failOnDataLoss", "false") \
                    .option("kafka.consumer.group.id", "spark-streaming-consumer") \
                    .load()
                
                print("âœ“ Kafka connection established!")
                print(f"Schema: {df_kafka.schema}")
                break
                
            except Exception as e:
                retry_count += 1
                if retry_count < max_retries:
                    print(f"âš  Kafka not ready yet (attempt {retry_count}/{max_retries}), retrying in 5s...")
                    time.sleep(5)
                else:
                    raise Exception(f"Failed to connect to Kafka after {max_retries} attempts: {str(e)}")
        
        # Xá»­ lÃ½ streaming vá»›i foreachBatch
        print("\nğŸ”„ Starting streaming query...")
        query = df_kafka \
            .writeStream \
            .foreachBatch(process_streaming_data) \
            .outputMode("update") \
            .trigger(processingTime="10 seconds") \
            .option("checkpointLocation", CHECKPOINT_LOCATION) \
            .start()
        
        print("=" * 70)
        print("âœ… Streaming query started successfully!")
        print("ğŸ“Š Waiting for data from Kafka...")
        print("ğŸ’¡ Data will be automatically written to Elasticsearch")
        print("ğŸŒ Access Kibana to visualize the data")
        print("=" * 70)
        print("\nPress Ctrl+C to stop the streaming job\n")
        
        # Chá» query cháº¡y
        query.awaitTermination()
        
    except KeyboardInterrupt:
        print("\n\nâš  Received interrupt signal. Stopping streaming...")
    except Exception as e:
        print(f"\nâŒ Error in streaming job: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        # Cleanup
        if query:
            try:
                print("\nğŸ›‘ Stopping streaming query...")
                query.stop()
            except:
                pass
        
        if spark:
            print("ğŸ›‘ Stopping Spark session...")
            spark.stop()
        
        print("âœ… Job completed.")
        print("=" * 70)


if __name__ == "__main__":
    main()

