# Setup M√¥i Tr∆∞·ªùng T·ª´ ƒê·∫ßu - Windows

## üöÄ Quick Start - Ch·∫°y 1 L·ªánh (Khuy·∫øn Ngh·ªã)

**N·∫øu b·∫°n mu·ªën setup t·ª± ƒë·ªông, ch·ªâ c·∫ßn:**

1. M·ªü **PowerShell as Administrator**
2. Navigate ƒë·∫øn th∆∞ m·ª•c project:
   ```powershell
   cd C:\BK_WORKSPACE\bigdata\bigdata_storage_and_proccess_job_data\bigdata-project\scripts\setup
   ```
3. Ch·∫°y script master:
   ```powershell
   .\run-all.ps1
   ```

Script s·∫Ω t·ª± ƒë·ªông:

- ‚úÖ C√†i ƒë·∫∑t t·∫•t c·∫£ prerequisites (Chocolatey, kubectl, Helm, Python...)
- ‚úÖ Start Minikube cluster
- ‚úÖ Sinh d·ªØ li·ªáu m·∫´u 20 file JSON
- ‚úÖ Deploy MinIO, Cassandra, Spark
- ‚úÖ Upload data v√†o MinIO
- ‚úÖ Init Cassandra schema
- ‚úÖ Build Spark image
- ‚úÖ Test Spark job
- ‚úÖ Deploy Airflow (optional)

**T·ªïng th·ªùi gian:** ~15-20 ph√∫t (t√πy t·ªëc ƒë·ªô m·∫°ng v√† m√°y t√≠nh)

---

## üìã Manual Setup (N·∫øu mu·ªën ki·ªÉm so√°t t·ª´ng b∆∞·ªõc)

### Y√™u C·∫ßu H·ªá Th·ªëng

- Windows 10/11
- RAM t·ªëi thi·ªÉu: 16GB (khuy·∫øn ngh·ªã 32GB)
- Disk: 50GB tr·ªëng
- CPU: 4 cores tr·ªü l√™n

## B∆∞·ªõc 1: C√†i ƒê·∫∑t C√°c Tool C·∫ßn Thi·∫øt

### T·ª± ƒê·ªông (Khuy·∫øn Ngh·ªã)

```powershell
cd bigdata-project\scripts\setup
.\1-install-prerequisites.ps1
```

### Th·ªß C√¥ng

### 1.1 C√†i Docker Desktop

1. Download Docker Desktop: https://www.docker.com/products/docker-desktop/
2. Ch·∫°y installer v√† l√†m theo h∆∞·ªõng d·∫´n
3. Kh·ªüi ƒë·ªông l·∫°i m√°y t√≠nh
4. M·ªü Docker Desktop ‚Üí Settings ‚Üí Resources:
   - CPUs: 4
   - Memory: 8GB
   - Swap: 2GB
   - Apply & Restart

**Ki·ªÉm tra:**

```bash
docker --version
docker run hello-world
```

### 1.2 C√†i Kubernetes (Minikube)

**C√†i Minikube:**

```bash
# Download Minikube installer
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-installer.exe

# Ch·∫°y installer
minikube-installer.exe
```

**Ho·∫∑c d√πng Chocolatey:**

```bash
# C√†i Chocolatey n·∫øu ch∆∞a c√≥ (ch·∫°y PowerShell as Admin)
Set-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))

# C√†i Minikube
choco install minikube
```

**Kh·ªüi ƒë·ªông Minikube:**

```bash
# Start v·ªõi driver Docker
minikube start --driver=docker --cpus=4 --memory=8192 --disk-size=50g

# Ki·ªÉm tra
minikube status
```

### 1.3 C√†i kubectl

```bash
# D√πng Chocolatey
choco install kubernetes-cli

# Ho·∫∑c download tr·ª±c ti·∫øp
curl.exe -LO "https://dl.k8s.io/release/v1.28.0/bin/windows/amd64/kubectl.exe"
# Copy kubectl.exe v√†o C:\Windows\System32\
```

**Ki·ªÉm tra:**

```bash
kubectl version --client
kubectl get nodes
```

### 1.4 C√†i Helm

```bash
# D√πng Chocolatey
choco install kubernetes-helm

# Ho·∫∑c download t·ª´: https://github.com/helm/helm/releases
```

**Ki·ªÉm tra:**

```bash
helm version
```

### 1.5 C√†i Python (cho script sinh data)

1. Download Python 3.11: https://www.python.org/downloads/
2. Ch·∫°y installer, **QUAN TR·ªåNG:** Tick ‚òëÔ∏è "Add Python to PATH"
3. Verify:

```bash
python --version
pip --version
```

### 1.6 C√†i Git (n·∫øu ch∆∞a c√≥)

```bash
choco install git

# Ho·∫∑c download: https://git-scm.com/download/win
```

## B∆∞·ªõc 2: Clone Project

```bash
cd C:\BK_WORKSPACE\bigdata
git clone <your-repo-url>
cd bigdata_storage_and_proccess_job_data\bigdata-project
```

## B∆∞·ªõc 3: T·∫°o D·ªØ Li·ªáu M·∫´u

```bash
cd scripts
python generate_sample_data.py

# K·∫øt qu·∫£: T·∫°o folder sample_data/ v·ªõi 20 file JSON
dir sample_data
```

## B∆∞·ªõc 4: Deploy Infrastructure tr√™n Kubernetes

### 4.1 Deploy MinIO

```bash
cd ..\k8s
kubectl apply -f minio-config.yaml

# ƒê·ª£i pods ready
kubectl wait --for=condition=ready pod -l app=minio --timeout=300s

# Ki·ªÉm tra status
kubectl get pods -l app=minio

# Port-forward ƒë·ªÉ truy c·∫≠p (m·ªü terminal m·ªõi)
kubectl port-forward svc/minio 9000:9000 9001:9001
```

**M·ªü tab browser m·ªõi:** http://localhost:9001

- Login: `minioadmin` / `minioadmin`
- Click "Buckets" ‚Üí "Create Bucket" ‚Üí T√™n: `sensor-data` ‚Üí Create
- Click v√†o bucket `sensor-data` ‚Üí "Upload" ‚Üí Browse ‚Üí Ch·ªçn 20 file JSON t·ª´ `scripts/sample_data/` ‚Üí Upload

### 4.2 Deploy Cassandra

```bash
kubectl apply -f cassandra-config.yaml

# ƒê·ª£i l√¢u h∆°n (2-3 ph√∫t)
kubectl wait --for=condition=ready pod -l app=cassandra --timeout=600s

# Ki·ªÉm tra cluster
kubectl exec -it cassandra-0 -- nodetool status
```

**Kh·ªüi t·∫°o schema:**

```bash
# Copy file CQL v√†o pod
kubectl cp init-cassandra.cql cassandra-0:/tmp/init-cassandra.cql

# Execute
kubectl exec -it cassandra-0 -- cqlsh -f /tmp/init-cassandra.cql

# Verify
kubectl exec -it cassandra-0 -- cqlsh -e "DESCRIBE KEYSPACE metrics;"
```

**B·∫°n s·∫Ω th·∫•y output:**

```
CREATE KEYSPACE metrics WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'};
CREATE TABLE metrics.avg_salary_by_experience ...
CREATE TABLE metrics.jobs_by_work_type ...
CREATE TABLE metrics.job_processing_audit ...
```

### 4.3 Deploy Spark

```bash
kubectl apply -f spark-config.yaml

# ƒê·ª£i Spark ready
kubectl wait --for=condition=ready pod -l app=spark-master --timeout=300s
kubectl wait --for=condition=ready pod -l app=spark-worker --timeout=300s

# Ki·ªÉm tra status
kubectl get pods -l app=spark-master
kubectl get pods -l app=spark-worker

# Xem Spark UI (m·ªü terminal m·ªõi)
kubectl port-forward svc/spark-master 8080:8080
```

M·ªü browser: http://localhost:8080 (s·∫Ω th·∫•y Spark Dashboard v·ªõi 2 workers)

## B∆∞·ªõc 5: Build Spark Job Image

### 5.1 C·∫•u h√¨nh Docker Registry

**Option A: D√πng Docker Hub (khuy·∫øn ngh·ªã cho beginner)**

1. T·∫°o t√†i kho·∫£n: https://hub.docker.com/
2. Login:

```bash
docker login
# Nh·∫≠p username v√† password
```

3. **QUAN TR·ªåNG:** Thay `your-docker-registry` trong c√°c file sau:

   **File 1: `k8s/spark-job-template.yaml`** (d√≤ng 18)

   ```yaml
   image: <your-docker-username>/spark-job:latest
   ```

   **File 2: `dags/job_processing_dag.py`** (d√≤ng 68)

   ```python
   image='<your-docker-username>/spark-job:latest',
   ```

**Option B: D√πng Minikube local registry (nhanh h∆°n cho test)**

```bash
# Enable registry addon
minikube addons enable registry

# Thi·∫øt l·∫≠p environment ƒë·ªÉ build tr·ª±c ti·∫øp trong Minikube
# PowerShell:
& minikube -p minikube docker-env --shell powershell | Invoke-Expression

# Ho·∫∑c CMD:
@FOR /f "tokens=*" %i IN ('minikube -p minikube docker-env --shell cmd') DO @%i

# D√πng image local
# Thay image th√†nh: localhost:5000/spark-job:latest
```

### 5.2 Build v√† Push Image

```bash
cd ..  # v·ªÅ th∆∞ m·ª•c bigdata-project

# Build (thay <your-docker-username> b·∫±ng username th·∫≠t c·ªßa b·∫°n)
docker build -f Dockerfile.spark -t <your-docker-username>/spark-job:latest .

# Push (n·∫øu d√πng Docker Hub)
docker push <your-docker-username>/spark-job:latest
```

**N·∫øu g·∫∑p l·ªói "cannot connect to Docker daemon":**

- M·ªü Docker Desktop
- ƒê·ª£i Docker Desktop running (icon m√†u xanh ·ªü system tray)
- Ch·∫°y l·∫°i l·ªánh build

**Build th√†nh c√¥ng khi th·∫•y:**

```
Successfully built xxxxx
Successfully tagged <your-username>/spark-job:latest
```

## B∆∞·ªõc 6: Test Spark Job Th·ªß C√¥ng

### 6.1 T·∫°o v√† Ch·∫°y Test Job

```bash
cd k8s

# PowerShell: T·∫°o file test v·ªõi date c·ª• th·ªÉ
(Get-Content spark-job-template.yaml) -replace 'DATEPLACEHOLDER', '2026-01-01' -replace 'your-docker-registry', '<your-docker-username>' | Set-Content spark-job-test.yaml

# CMD: Ho·∫∑c d√πng text editor s·ª≠a th·ªß c√¥ng
# M·ªü spark-job-template.yaml, thay DATEPLACEHOLDER ‚Üí 2026-01-01
# Thay your-docker-registry ‚Üí <your-docker-username>
# Save as spark-job-test.yaml

# Apply job
kubectl apply -f spark-job-test.yaml

# Ki·ªÉm tra job status
kubectl get jobs
```

### 6.2 Xem Logs v√† Debug

```bash
# Xem logs real-time
kubectl logs -f job/spark-job-2026-01-01

# N·∫øu job ch∆∞a start, xem pod events
kubectl describe job spark-job-2026-01-01

# N·∫øu pod pending, ki·ªÉm tra
kubectl get pods -l job-name=spark-job-2026-01-01
kubectl describe pod <pod-name>
```

**Logs th√†nh c√¥ng s·∫Ω c√≥:**

```
Reading data from: s3a://sensor-data/jobs_2026-01-01.json
Read XX records from ...
Written X records to avg_salary_by_experience
Written Y records to jobs_by_work_type
‚úì Job completed successfully for 2026-01-01
```

### 6.3 Ki·ªÉm Tra K·∫øt Qu·∫£ trong Cassandra

```bash
# Query k·∫øt qu·∫£ l∆∞∆°ng TB theo experience
kubectl exec -it cassandra-0 -- cqlsh -e "SELECT * FROM metrics.avg_salary_by_experience WHERE process_date='2026-01-01' ALLOW FILTERING;"

# Query s·ªë job theo work type
kubectl exec -it cassandra-0 -- cqlsh -e "SELECT * FROM metrics.jobs_by_work_type WHERE process_date='2026-01-01' ALLOW FILTERING;"

# Xem audit log
kubectl exec -it cassandra-0 -- cqlsh -e "SELECT job_name, status, records_read, records_written_exp, records_written_work, started_at FROM metrics.job_processing_audit ORDER BY started_at DESC LIMIT 5 ALLOW FILTERING;"
```

**N·∫øu th·∫•y data ‚Üí Th√†nh c√¥ng!** ‚úÖ

## B∆∞·ªõc 7: Deploy Airflow (T·ª± ƒê·ªông H√≥a)

### 7.1 C√†i Airflow b·∫±ng Helm

```bash
# Add repo
helm repo add apache-airflow https://airflow.apache.org
helm repo update

# T·∫°o namespace
kubectl create namespace airflow
```

### 7.2 T·∫°o File C·∫•u H√¨nh Airflow

T·∫°o file `airflow-values.yaml` trong th∆∞ m·ª•c `bigdata-project/`:

```yaml
executor: KubernetesExecutor

# T·∫Øt git-sync ƒë·ªÉ mount DAGs t·ª´ local
dags:
  gitSync:
    enabled: false

logs:
  persistence:
    enabled: true
    size: 5Gi

webserver:
  service:
    type: NodePort

env:
  - name: AIRFLOW__CORE__LOAD_EXAMPLES
    value: "False"
  - name: AIRFLOW__KUBERNETES__IN_CLUSTER
    value: "True"
  - name: AIRFLOW__KUBERNETES__NAMESPACE
    value: "default"
  - name: AIRFLOW__CORE__DAGS_FOLDER
    value: "/opt/airflow/dags"

# Resources
resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi
```

### 7.3 Mount DAGs v√†o Airflow (C√°ch ƒê∆°n Gi·∫£n)

**C√°ch 1: Copy DAG v√†o ConfigMap (khuy·∫øn ngh·ªã cho test)**

```bash
# T·∫°o ConfigMap t·ª´ DAG file
kubectl create configmap airflow-dags --from-file=dags/job_processing_dag.py -n airflow

# Update values.yaml th√™m:
# extraVolumes:
#   - name: dags
#     configMap:
#       name: airflow-dags
# extraVolumeMounts:
#   - name: dags
#     mountPath: /opt/airflow/dags
```

**C√°ch 2: Rebuild Airflow image c√≥ s·∫µn DAG**

T·∫°o `Dockerfile.airflow`:

```dockerfile
FROM apache/airflow:2.7.0
COPY dags/ /opt/airflow/dags/
```

Build v√† push:

```bash
docker build -f Dockerfile.airflow -t <your-username>/airflow-custom:latest .
docker push <your-username>/airflow-custom:latest

# Update values.yaml:
# images:
#   airflow:
#     repository: <your-username>/airflow-custom
#     tag: latest
```

### 7.4 Install Airflow

```bash
helm install airflow apache-airflow/airflow -n airflow -f airflow-values.yaml --timeout 10m

# ƒê·ª£i ready (c√≥ th·ªÉ m·∫•t 5-10 ph√∫t)
kubectl get pods -n airflow -w
# Ctrl+C khi t·∫•t c·∫£ pods ƒë√£ Running

# Ki·ªÉm tra status
kubectl get pods -n airflow
```

### 7.5 Truy c·∫≠p Airflow UI

```bash
# Get NodePort
kubectl get svc -n airflow airflow-webserver

# Ho·∫∑c port-forward (d·ªÖ h∆°n)
kubectl port-forward svc/airflow-webserver 8081:8080 -n airflow
```

M·ªü browser: http://localhost:8081

**Login:**

- Username: `admin`
- Password: L·∫•y b·∫±ng l·ªánh:

```bash
kubectl get secret airflow-webserver-secret -n airflow -o jsonpath="{.data.webserver-secret-key}" | base64 -d
```

### 7.6 Setup Airflow Variable

1. V√†o Airflow UI ‚Üí Admin ‚Üí Variables
2. Click "+ Add a new record"
3. Nh·∫≠p:
   - **Key:** `job_processing_current_date`
   - **Val:** `2026-01-01`
4. Save

### 7.7 Enable DAG

1. V√†o Airflow UI ‚Üí DAGs
2. T√¨m `job_processing_pipeline`
3. Toggle switch t·ª´ OFF ‚Üí ON
4. DAG s·∫Ω t·ª± ch·∫°y m·ªói 5 ph√∫t

## B∆∞·ªõc 8: Monitoring & Debugging

### 8.1 Check T·∫•t C·∫£ Pods

```bash
# Xem t·∫•t c·∫£ pods trong cluster
kubectl get pods -A

# Xem pods theo namespace
kubectl get pods -n default
kubectl get pods -n airflow

# Xem chi ti·∫øt pod
kubectl describe pod <pod-name>
```

### 8.2 Access Services

**MinIO Console:**

```bash
kubectl port-forward svc/minio 9001:9001
# http://localhost:9001
```

**Spark UI:**

```bash
kubectl port-forward svc/spark-master 8080:8080
# http://localhost:8080
```

**Airflow UI:**

```bash
kubectl port-forward svc/airflow-webserver 8081:8080 -n airflow
# http://localhost:8081
```

### 8.3 Query Cassandra

```bash
# Exec v√†o pod ƒë·ªÉ m·ªü cqlsh
kubectl exec -it cassandra-0 -- cqlsh

# Trong cqlsh, ch·∫°y queries:
USE metrics;

-- Xem l∆∞∆°ng TB theo experience
SELECT experience_level, avg_salary, job_count, process_date
FROM avg_salary_by_experience
LIMIT 20;

-- Xem s·ªë job theo work type
SELECT work_type, job_count, process_date
FROM jobs_by_work_type
LIMIT 20;

-- Xem audit logs
SELECT process_date, status, records_read, started_at, completed_at
FROM job_processing_audit
ORDER BY started_at DESC
LIMIT 10
ALLOW FILTERING;

-- Exit
exit
```

### 8.4 Xem Logs Airflow DAG Run

```bash
# Xem logs scheduler
kubectl logs -f -n airflow -l component=scheduler

# Xem logs c·ªßa m·ªôt task c·ª• th·ªÉ (trong Airflow UI)
# DAGs ‚Üí job_processing_pipeline ‚Üí Click v√†o run ‚Üí Click v√†o task ‚Üí View Log
```

## Troubleshooting Ph·ªï Bi·∫øn

### 1. Minikube kh√¥ng start ƒë∆∞·ª£c

```bash
# Xem l·ªói chi ti·∫øt
minikube start --driver=docker --cpus=4 --memory=8192 -v=7

# N·∫øu v·∫´n l·ªói, reset ho√†n to√†n
minikube delete --all --purge
minikube start --driver=docker --cpus=4 --memory=8192

# Check Docker Desktop ƒëang ch·∫°y
docker ps
```

### 2. Pods b·ªã Pending

```bash
# Xem t·∫°i sao pending
kubectl describe pod <pod-name>

# Th∆∞·ªùng do:
# - Insufficient memory/cpu ‚Üí Gi·∫£m resources trong config
# - Image pull failed ‚Üí Check image name, registry credentials
# - Volume mount failed ‚Üí Check volume exists
```

### 3. Pods b·ªã CrashLoopBackOff

```bash
# Xem logs l·ªói
kubectl logs <pod-name>
kubectl logs <pod-name> --previous  # Logs c·ªßa l·∫ßn restart tr∆∞·ªõc

# Xem events
kubectl get events --sort-by=.metadata.creationTimestamp | grep <pod-name>
```

### 4. Docker build l·ªói "cannot connect"

```bash
# Check Docker Desktop running
docker ps

# Restart Docker Desktop
# Right-click icon ‚Üí Restart

# Verify
docker run hello-world
```

### 5. Spark job l·ªói "FileNotFoundException: s3a://sensor-data/..."

**Nguy√™n nh√¢n:** MinIO ch∆∞a c√≥ data ho·∫∑c credentials sai

```bash
# Check MinIO pods running
kubectl get pods -l app=minio

# Check bucket exists
kubectl port-forward svc/minio 9001:9001
# V√†o http://localhost:9001 ki·ªÉm tra bucket sensor-data v√† files

# Test connection t·ª´ Spark pod
kubectl run -it --rm debug --image=alpine --restart=Never -- sh
# Trong pod:
apk add curl
curl http://minio.default.svc.cluster.local:9000
# N·∫øu k·∫øt n·ªëi ƒë∆∞·ª£c s·∫Ω th·∫•y XML response
```

### 6. Spark job l·ªói "Connection refused to Cassandra"

```bash
# Check Cassandra pods running
kubectl get pods -l app=cassandra

# Check Cassandra ready
kubectl exec -it cassandra-0 -- nodetool status
# Output: UN (Up Normal) cho c·∫£ 3 nodes

# Test connection
kubectl run -it --rm debug --image=alpine --restart=Never -- sh
# Trong pod:
apk add curl
nc -zv cassandra.default.svc.cluster.local 9042
# Connection successful n·∫øu k·∫øt n·ªëi ƒë∆∞·ª£c
```

### 7. Cassandra kh√¥ng ready sau 10 ph√∫t

```bash
# Xem logs
kubectl logs cassandra-0 | tail -100

# Th∆∞·ªùng do: Insufficient resources
# Gi·∫£m replicas trong cassandra-config.yaml:
# replicas: 1  # thay v√¨ 3

# Redeploy
kubectl delete -f k8s/cassandra-config.yaml
kubectl apply -f k8s/cassandra-config.yaml
```

### 8. Airflow DAG kh√¥ng hi·ªán

```bash
# Check DAG file syntax
python dags/job_processing_dag.py
# Kh√¥ng c√≥ l·ªói = OK

# Xem logs scheduler
kubectl logs -n airflow -l component=scheduler | grep ERROR

# Refresh DAGs (trong Airflow UI)
# DAGs page ‚Üí Click refresh icon

# Ho·∫∑c restart scheduler
kubectl rollout restart deployment airflow-scheduler -n airflow
```

### 9. Image pull error: "unauthorized" ho·∫∑c "not found"

```bash
# Verify image exists in registry
docker images | grep spark-job

# Check image name ƒë√∫ng trong:
# - k8s/spark-job-template.yaml
# - dags/job_processing_dag.py

# N·∫øu d√πng Docker Hub, ƒë·∫£m b·∫£o image l√† public
# Ho·∫∑c t·∫°o ImagePullSecret:
kubectl create secret docker-registry regcred \
  --docker-server=https://index.docker.io/v1/ \
  --docker-username=<your-username> \
  --docker-password=<your-password>

# Th√™m v√†o pod spec:
# imagePullSecrets:
#   - name: regcred
```

### 10. Port-forward b·ªã disconnect

```bash
# Th√™m flag --address ƒë·ªÉ bind t·∫•t c·∫£ interfaces
kubectl port-forward --address 0.0.0.0 svc/minio 9001:9001

# Ho·∫∑c run trong background (PowerShell)
Start-Process kubectl -ArgumentList "port-forward svc/minio 9001:9001" -WindowStyle Hidden
```

## Checklist Ho√†n Th√†nh Setup

ƒê√°nh d·∫•u khi ho√†n th√†nh t·ª´ng b∆∞·ªõc:

### Prerequisites

- [ ] Docker Desktop ƒë√£ c√†i v√† ch·∫°y (`docker ps` th√†nh c√¥ng)
- [ ] Minikube ƒë√£ start (`minikube status` = Running)
- [ ] kubectl connect ƒë∆∞·ª£c cluster (`kubectl get nodes` = Ready)
- [ ] Helm ƒë√£ c√†i (`helm version`)
- [ ] Python ƒë√£ c√†i (`python --version`)

### Data Preparation

- [ ] ƒê√£ t·∫°o 20 file JSON trong `sample_data/`
- [ ] File JSON c√≥ ƒë√∫ng format (m·ªü file ki·ªÉm tra)

### Infrastructure

- [ ] MinIO pods ƒëang Running (`kubectl get pods -l app=minio`)
- [ ] Cassandra pods ƒëang Running (`kubectl get pods -l app=cassandra`)
- [ ] Spark Master + Workers ƒëang Running (`kubectl get pods | grep spark`)

### MinIO Setup

- [ ] Truy c·∫≠p ƒë∆∞·ª£c MinIO console (http://localhost:9001)
- [ ] ƒê√£ t·∫°o bucket `sensor-data`
- [ ] ƒê√£ upload 20 file JSON v√†o bucket
- [ ] Verify: V√†o bucket th·∫•y 20 files

### Cassandra Setup

- [ ] ƒê√£ init schema (`kubectl exec cassandra-0 -- cqlsh -e "DESCRIBE KEYSPACE metrics;"`)
- [ ] Th·∫•y 3 tables: avg_salary_by_experience, jobs_by_work_type, job_processing_audit

### Spark Job

- [ ] Build image th√†nh c√¥ng
- [ ] Push image th√†nh c√¥ng (ho·∫∑c load v√†o Minikube)
- [ ] Test job ch·∫°y th√†nh c√¥ng (`kubectl logs job/spark-job-2026-01-01`)
- [ ] Query Cassandra th·∫•y data cho date 2026-01-01

### Airflow (Optional)

- [ ] Airflow pods ƒëang Running (`kubectl get pods -n airflow`)
- [ ] Truy c·∫≠p ƒë∆∞·ª£c Airflow UI (http://localhost:8081)
- [ ] ƒê√£ set Variable `job_processing_current_date`
- [ ] DAG `job_processing_pipeline` ƒë√£ enable
- [ ] DAG ch·∫°y th√†nh c√¥ng √≠t nh·∫•t 1 l·∫ßn

## L·ªánh H·ªØu √çch

```bash
# Restart t·∫•t c·∫£ pods c·ªßa m·ªôt service
kubectl rollout restart deployment <deployment-name>
kubectl rollout restart statefulset <statefulset-name>

# Delete v√† redeploy
kubectl delete -f <file>.yaml
kubectl apply -f <file>.yaml

# Xem resource usage
kubectl top nodes
kubectl top pods -A

# Port forward nhi·ªÅu services c√πng l√∫c (PowerShell)
Start-Process kubectl -ArgumentList "port-forward svc/minio 9001:9001"
Start-Process kubectl -ArgumentList "port-forward svc/spark-master 8080:8080"
Start-Process kubectl -ArgumentList "port-forward svc/airflow-webserver 8081:8080 -n airflow"

# Clean up resource ƒë·ªÉ free memory
kubectl delete job --all  # X√≥a completed jobs
kubectl delete pod --field-selector=status.phase==Succeeded  # X√≥a succeeded pods

# Reset ho√†n to√†n ƒë·ªÉ b·∫Øt ƒë·∫ßu l·∫°i
kubectl delete namespace airflow
kubectl delete all --all
minikube delete
minikube start --driver=docker --cpus=4 --memory=8192
```

## Performance Tips

### N·∫øu m√°y y·∫øu (8GB RAM)

```bash
# Start Minikube v·ªõi √≠t resources h∆°n
minikube start --driver=docker --cpus=2 --memory=6144

# Gi·∫£m replicas trong configs:
# cassandra-config.yaml: replicas: 1
# spark-config.yaml: workers replicas: 1

# Gi·∫£m Spark executor memory trong spark_job.py:
# .config("spark.executor.memory", "1g")
```

### N·∫øu build/pull image ch·∫≠m

```bash
# D√πng Minikube cache
minikube cache add apache/spark:3.5.1
minikube cache add cassandra:4.1
minikube cache add minio/minio:RELEASE.2023-09-30T07-02-29Z

# Build tr·ª±c ti·∫øp trong Minikube (kh√¥ng c·∫ßn push)
eval $(minikube docker-env)  # Linux/Mac
minikube docker-env | Invoke-Expression  # PowerShell
docker build -f Dockerfile.spark -t spark-job:latest .
# S·ª≠a imagePullPolicy: Never trong yaml files
```

## Next Steps: Production Deployment

Sau khi test th√†nh c√¥ng tr√™n local, ƒë·ªÉ deploy production:

1. **Setup Git Repository**

   - Push code l√™n GitHub/GitLab
   - Setup Git-Sync trong Airflow ƒë·ªÉ t·ª± ƒë·ªông sync DAGs

2. **Use Cloud Kubernetes**

   - Azure AKS
   - AWS EKS
   - Google GKE

3. **Persistent Storage**

   - Replace emptyDir v·ªõi PersistentVolumeClaim
   - Use cloud storage (Azure Blob, S3) cho MinIO data

4. **Monitoring**

   - Deploy Prometheus + Grafana
   - Setup alerts cho job failures

5. **Security**

   - Use secrets management (Azure Key Vault, AWS Secrets Manager)
   - Setup RBAC
   - Network policies

6. **CI/CD**
   - GitHub Actions ƒë·ªÉ auto build/push images
   - ArgoCD ho·∫∑c Flux cho GitOps

---

**Ch√∫c b·∫°n setup th√†nh c√¥ng! üöÄ**

N·∫øu g·∫∑p l·ªói kh√¥ng c√≥ trong troubleshooting, h√£y:

1. Copy full error message
2. Check logs: `kubectl logs <pod-name>`
3. Check events: `kubectl describe pod <pod-name>`
4. Google error message
5. H·ªèi tr√™n Stack Overflow ho·∫∑c Kubernetes Slack
