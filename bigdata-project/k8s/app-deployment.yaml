# --- Multi-Source Producer: Lấy dữ liệu từ LinkedIn & Adzuna ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: producer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: producer
  template:
    metadata:
      labels:
        app: producer
    spec:
      containers:
      - name: producer
        image: baokieu/my-repo:v1
        imagePullPolicy: Always
        env:
        - name: PYTHONUNBUFFERED
          value: "1"
        command: ["python3", "/app/src/producer_v2.py"]

---
# --- Ingestor: Đọc Kafka -> Ghi MinIO ---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingestor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ingestor
  template:
    metadata:
      labels:
        app: ingestor
    spec:
      containers:
        - name: ingestor
          image: baokieu/my-repo:v1
          imagePullPolicy: Always
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
          command: ["python3", "/app/src/kafka_to_minio.py"]

---
# 3. Spark Streaming (Kafka -> Elasticsearch)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spark-streaming
spec:
  replicas: 1
  selector:
    matchLabels:
      app: spark-streaming
  template:
    metadata:
      labels:
        app: spark-streaming
    spec:
      containers:
      - name: spark
        image: baokieu/my-repo:v1
        imagePullPolicy: Always
        command: ["/bin/bash", "-c"]
        args:
        - |
          # 1. Lấy IP của Pod hiện tại (Driver)
          MY_POD_IP=$(hostname -i)
          echo ">>> Driver IP is: $MY_POD_IP"

          # # 2. Truyền IP đó vào cấu hình Spark để Worker biết đường kết nối lại
          /opt/spark/bin/spark-submit \
          --master spark://spark-master:7077 \
          --deploy-mode client  \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.1 \
          --conf spark.driver.host=$MY_POD_IP \
          --conf spark.driver.bindAddress=0.0.0.0 \
          --conf spark.driver.port=30000 \
          --conf spark.blockManager.port=30001 \
          /app/src/spark_streaming_v2.py
      restartPolicy: Always
---
# --- Spark Processor: Xử lý data (MinIO -> Cassandra & Elasticsearch) ---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: spark-processor
spec:
  schedule: "*/10 * * * *"     # Chạy định kỳ 1 phút/lần
  concurrencyPolicy: Forbid   # Không cho phép 2 job chạy chồng chéo
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: spark
            image: baokieu/my-repo:v1
            imagePullPolicy: Always
            command: ["/bin/bash", "-c"]
            args:
            - |
              # 1. Lấy IP của Pod hiện tại (Driver)
              MY_POD_IP=$(hostname -i)
              echo ">>> Driver IP is: $MY_POD_IP"

              # # 2. Truyền IP đó vào cấu hình Spark để Worker biết đường kết nối lại
              /opt/spark/bin/spark-submit \
              --master spark://spark-master:7077 \
              --deploy-mode client \
              --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.elasticsearch:elasticsearch-spark-30_2.12:8.11.1 \
              --conf spark.driver.host=$MY_POD_IP \
              --conf spark.driver.bindAddress=0.0.0.0 \
              --conf spark.driver.port=30000 \
              --conf spark.blockManager.port=30001 \
              /app/src/spark_batch_v2.py
          restartPolicy: OnFailure
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: job-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: job-api
  template:
    metadata:
      labels:
        app: job-api
    spec:
      containers:
      - name: job-api 
        image: baokieu/job_api:v1
        imagePullPolicy: Always
        ports:
          - containerPort: 8080
        env:
          - name: DATA_FILE_PATH
            value: "/app/data.json"
---
apiVersion: v1
kind: Service
metadata:
  name: job-api-svc
spec:
  selector:
    app: job-api
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  type: ClusterIP
