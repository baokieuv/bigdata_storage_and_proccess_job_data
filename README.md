# ğŸš€ Deploy K8s Full Big Data Project

## ğŸ“Œ Tá»•ng quan

Project nÃ y triá»ƒn khai há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u Big Data end-to-end trÃªn Kubernetes, bao gá»“m:

- **Kafka (Strimzi)**: Ingest dá»¯ liá»‡u job
- **MinIO**: LÆ°u trá»¯ dá»¯ liá»‡u thÃ´ (raw data)
- **Spark**:
  - Streaming: Kafka â†’ Elasticsearch
  - Batch: MinIO â†’ Cassandra + Elasticsearch
- **Cassandra**: LÆ°u trá»¯ dá»¯ liá»‡u phÃ¢n tÃ­ch
- **Elasticsearch + Kibana**: Realtime & analytics visualization
- **Python services**: Producer, ingestor
- **Docker + Kubernetes**: GKE / kind

### Luá»“ng xá»­ lÃ½ dá»¯ liá»‡u

```
Job API â†’ Producer â†’ Kafka
Kafka â†’ Spark Streaming â†’ Elasticsearch (Realtime)
Kafka â†’ MinIO â†’ Spark Batch â†’ Cassandra + Elasticsearch
```

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
.
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ kafka-config.yaml
â”‚   â”œâ”€â”€ minio-config.yaml
â”‚   â”œâ”€â”€ cassandra-config.yaml
â”‚   â”œâ”€â”€ spark-config.yaml
â”‚   â”œâ”€â”€ elastic-config.yaml
â”‚   â”œâ”€â”€ kibana-config.yaml
â”‚   â”œâ”€â”€ init-job.yaml
â”‚   â”œâ”€â”€ app-deployment.yaml
â”‚   â””â”€â”€ kind-config.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ producer.py
â”‚   â”œâ”€â”€ kafka_to_minio.py
â”‚   â”œâ”€â”€ spark_job.py
â”‚   â””â”€â”€ spark_streaming.py
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

## I. Chuáº©n bá»‹ mÃ´i trÆ°á»ng

### 1ï¸âƒ£ Cháº¡y trÃªn Google Cloud (GKE)

```bash
gcloud auth login
gcloud config set project YOUR_PROJECT_ID
gcloud services enable container.googleapis.com artifactregistry.googleapis.com
```

**Táº¡o cluster 3 nodes:**

> **Khuyáº¿n nghá»‹**: machine type `e2-standard-2` (2 vCPU, 8GB RAM) do dÃ¹ng Kafka + Cassandra + Spark

**Kiá»ƒm tra káº¿t ná»‘i:**

```bash
kubectl get nodes
```

### 2ï¸âƒ£ Cháº¡y local báº±ng kind

Táº¡o file `k8s/kind-config.yaml`:

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
  - role: worker
```

Khá»Ÿi táº¡o cluster:

```bash
kind create cluster --name bigdata-cluster --config k8s/kind-config.yaml
kubectl get nodes
```

## II. Deploy háº¡ táº§ng Big Data

### 1ï¸âƒ£ CÃ i Strimzi (Kafka Operator)

```bash
kubectl create -f 'https://strimzi.io/install/latest?namespace=default'
```

> âš ï¸ **Chá» Strimzi running xong trÆ°á»›c khi deploy Kafka**

### 2ï¸âƒ£ Deploy Kafka

```bash
kubectl apply -f k8s/kafka-config.yaml
```

- Kafka cháº¡y cháº¿ Ä‘á»™ KRaft
- 3 replicas (cÃ³ thá»ƒ giáº£m khi test)

### 3ï¸âƒ£ Deploy MinIO (Distributed)

```bash
kubectl apply -f k8s/minio-config.yaml
```

- MinIO cháº¡y StatefulSet
- Tá»‘i thiá»ƒu 4 replicas (Erasure Coding)
- Bucket dÃ¹ng trong project: `job-raw-data`

### 4ï¸âƒ£ Deploy Cassandra

```bash
kubectl apply -f k8s/cassandra-config.yaml
```

- 3 replicas
- CÃ³ readiness probe
- DÃ¹ng CQL port 9042

### 5ï¸âƒ£ Deploy Spark Cluster

```bash
kubectl apply -f k8s/spark-config.yaml
```

- 1 Spark Master
- 3 Spark Workers (cÃ³ thá»ƒ giáº£m)

### 6ï¸âƒ£ Deploy Elasticsearch & Kibana

```bash
kubectl apply -f k8s/elastic-config.yaml
kubectl apply -f k8s/kibana-config.yaml
```

### 7ï¸âƒ£ Init resource (bucket + Cassandra table)

> âš ï¸ **Chá»‰ cháº¡y sau khi MinIO & Cassandra Ä‘Ã£ RUNNING**

```bash
kubectl apply -f k8s/init-job.yaml
```

Sau khi hoÃ n táº¥t:

```bash
kubectl delete -f k8s/init-job.yaml
```

## III. Build & Push Docker Image

### 1ï¸âƒ£ Build image

```bash
docker build -t <dockerhub-username>/<repo>:v1 .
```

### 2ï¸âƒ£ Push image

```bash
docker push <dockerhub-username>/<repo>:v1
```

## IV. Deploy cÃ¡c á»©ng dá»¥ng xá»­ lÃ½ dá»¯ liá»‡u

**File**: `k8s/app-deployment.yaml`

Bao gá»“m:

- **Producer**: Job API â†’ Kafka
- **Ingestor**: Kafka â†’ MinIO
- **Spark Streaming**: Kafka â†’ Elasticsearch
- **Spark Batch (CronJob)**: MinIO â†’ Cassandra + Elasticsearch
- **Job API**

> âš ï¸ **Sá»­a image name trong file:**

```yaml
image: baokieu/my-repo:v1   # THAY Báº°NG IMAGE Cá»¦A Báº N
```

**Deploy:**

```bash
kubectl apply -f k8s/app-deployment.yaml
```

## V. MÃ´ táº£ cÃ¡c chÆ°Æ¡ng trÃ¬nh

### ğŸ”¹ producer.py

- Gá»i Job API
- Gá»­i dá»¯ liá»‡u vÃ o Kafka topic `jobs-topic`
- Retry náº¿u Kafka chÆ°a sáºµn sÃ ng

### ğŸ”¹ kafka_to_minio.py

- Consume Kafka
- Gom batch:
  - 10 records hoáº·c
  - 60s
- Ghi file JSON vÃ o MinIO

### ğŸ”¹ spark_streaming.py

- Spark Structured Streaming
- Kafka â†’ Elasticsearch
- Index: `jobs-realtime`

### ğŸ”¹ spark_job.py

- Spark Batch
- Äá»c dá»¯ liá»‡u tá»« MinIO theo batch
- TÃ­nh:
  - job_count
  - avg / max / min salary
- Ghi vÃ o:
  - Cassandra (`job_metrics.company_analytics`)
  - Elasticsearch (`jobs-analytics`)
- CÃ³ state file Ä‘á»ƒ trÃ¡nh xá»­ lÃ½ trÃ¹ng

## VI. Kiá»ƒm tra há»‡ thá»‘ng

### Xem log

```bash
kubectl logs <pod-name>
```

VÃ­ dá»¥:

```bash
kubectl logs spark-master-xxx
kubectl logs spark-processor-xxx
```

### Kiá»ƒm tra Cassandra

```bash
kubectl exec -it cassandra-0 -- cqlsh
```

```sql
SELECT * FROM job_metrics.company_analytics;
```

### Truy cáº­p Kibana

```bash
kubectl port-forward svc/kibana 5601:5601
```

Má»Ÿ trÃ¬nh duyá»‡t:

```
http://localhost:5601
```

## ğŸ“Œ Ghi chÃº quan trá»ng

- CÃ³ thá»ƒ giáº£m replicas khi cháº¡y local
- MinIO distributed báº¯t buá»™c â‰¥ 4 pod
- Spark Streaming dÃ¹ng deploy-mode `client` Ä‘á»ƒ dá»… debug
- CronJob batch cháº¡y má»—i 2 phÃºt

## âœ… CÃ´ng nghá»‡ sá»­ dá»¥ng

- Kubernetes
- Kafka (Strimzi)
- Apache Spark
- MinIO
- Cassandra
- Elasticsearch + Kibana
- Docker
- Python

---

**Happy Big Data Processing! ğŸ‰**
